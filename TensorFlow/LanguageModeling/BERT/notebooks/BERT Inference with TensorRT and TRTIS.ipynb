{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# End-to-End BERT (Inference)\n",
    "\n",
    "이 문서는 TensorFlow BERT pretrained weight를 TensorRT engine으로 변환한 이후, TRTIS에 연동해서 Inference Serving하는 방법을 안내하기 위해 작성이 되었습니다. 이 문서에서 처리하는 절차는 크게 다음 두 가지 입니다.\n",
    "\n",
    "**1. BERT TensorRT inference engine build**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/deeplearning/tensorrt/trt-info.png\" width=\"600\" />\n",
    "TensorFlow이용하여 학습된 BERT pretrained weight를 TensorRT engine 파일로 변환합니다. 이 예제에서는 이 과정에서 필요한 plugin 들을 build하는 과정도 포함합니다. TensorRT를 이용한 BERT Sample에 대한 자세한 설명을 보시려면, [Real-Time Natural Language Understanding with BERT Using TensorRT](https://devblogs.nvidia.com/nlu-with-tensorrt-bert/)를 참고하세요.\n",
    "\n",
    "**2. TRTIS model repository 구성 및 TRTIS 서버 실행**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/pictures/2018/trt-inference-server-diagram-1200px.png\" width=\"600\" />\n",
    "이 예제에서는 완성된 engine 파일을 이용하여, TRTIS용 Model repository를 구성하고, TensorRT Inference Server를 실제로 구동하여 동작하는 것을 살펴볼 것입니다. TensorRT Inference Server에 대한 자세한 설명을 보시려면, [NVIDIA TensorRT Inference Server Boosts Deep Learning Inference](https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/) 문서를 참고하세요.\n",
    "\n",
    "이 문서는 DGX-1 V100 16GB 장비를 이용하여 테스트 되었으며, 구동 환경에 따라 성능은 다를 수 있습니다. 또한 사용된 SW는 다음과 같습니다.\n",
    "* CUDA 10.1 / CUDNN 7 / TensorRT 6.0\n",
    "* NGC Containers\n",
    "\n",
    "| 용도 | NGC container |\n",
    "|:---:|:---:|\n",
    "| TensorRT engine build | cuda:10.1-cudnn7-devel-ubuntu18.04 |\n",
    "| TensorRT Inference Server | tensorrtserver:12.10-py3 |\n",
    "| BERT inference client | tensorflow:12.08-py3 |\n",
    "    \n",
    "* docker / nvidia-docker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Building BERT TensorRT Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build TensorRT Docker Container\n",
    "\n",
    "TensorRT engine을 build하기 위해 우선 build 환경을 구축하기 위한 docker image를 생성합니다. 여기서는 TensorRT 6.0 BERT inference 예제를 이용할 것이며, 기준 환경인 CUDA 10.1 / TensorRT 6.0을 이용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  69.86MB\r",
      "\r\n",
      "Step 1/18 : ARG FROM_IMAGE_NAME\n",
      "Step 2/18 : FROM ${FROM_IMAGE_NAME}\n",
      " ---> b4879c167fc1\n",
      "Step 3/18 : ARG myuid\n",
      " ---> Using cache\n",
      " ---> 6a62c3c927aa\n",
      "Step 4/18 : ARG mygid\n",
      " ---> Using cache\n",
      " ---> 6f82f31498b7\n",
      "Step 5/18 : RUN echo $myuid $mygid\n",
      " ---> Using cache\n",
      " ---> 47a92067d609\n",
      "Step 6/18 : RUN groupadd -r --gid ${mygid} bert && useradd -r -u ${myuid} --gid ${mygid} -ms /bin/bash bert\n",
      " ---> Using cache\n",
      " ---> f4ee281105eb\n",
      "Step 7/18 : RUN echo 'bert:bert' | chpasswd\n",
      " ---> Using cache\n",
      " ---> 8ccc1b7fa11c\n",
      "Step 8/18 : RUN mkdir -p /workspace && chown -R bert /workspace\n",
      " ---> Using cache\n",
      " ---> 913034d1d28e\n",
      "Step 9/18 : ARG TRT_PKG_VERSION=7.0.0-1+cuda10.2\n",
      " ---> Using cache\n",
      " ---> 52f8e83a3ab1\n",
      "Step 10/18 : RUN apt-get update && apt-get install -y --no-install-recommends     software-properties-common     pbzip2 pv bzip2  sudo gcc-7 g++-7  zlib1g-dev      unzip     libcurl4-openssl-dev     wget     zlib1g-dev     git     pkg-config     python3     python3-dev     python3-pip     python3-setuptools     python3-wheel     libnvinfer6=${TRT_PKG_VERSION}     libnvinfer-dev=${TRT_PKG_VERSION}     libnvinfer-plugin6=${TRT_PKG_VERSION}     libnvinfer-plugin-dev=${TRT_PKG_VERSION}     libnvparsers6=${TRT_PKG_VERSION}     libnvparsers-dev=${TRT_PKG_VERSION}     libnvonnxparsers6=${TRT_PKG_VERSION}     libnvonnxparsers-dev=${TRT_PKG_VERSION}     python3-libnvinfer=${TRT_PKG_VERSION}     python3-libnvinfer-dev=${TRT_PKG_VERSION}\n",
      " ---> Using cache\n",
      " ---> f7707c1c89ba\n",
      "Step 11/18 : RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60                                  --slave /usr/bin/g++ g++ /usr/bin/g++-7  &&                                  update-alternatives --config gcc\n",
      " ---> Using cache\n",
      " ---> ffd0ff45aaf7\n",
      "Step 12/18 : RUN cd /tmp &&   wget https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh &&   chmod +x cmake-3.14.4-Linux-x86_64.sh &&   ./cmake-3.14.4-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir --skip-license &&   rm ./cmake-3.14.4-Linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> 4333337daa55\n",
      "Step 13/18 : RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1 &&     update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 84c1c71fe8f1\n",
      "Step 14/18 : RUN pip install tensorflow==1.14.0 &&     pip install pycuda\n",
      " ---> Using cache\n",
      " ---> 669f44c81903\n",
      "Step 15/18 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 521dbc66cb30\n",
      "Step 16/18 : USER bert\n",
      " ---> Using cache\n",
      " ---> e9d4e93c086b\n",
      "Step 17/18 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 7493a7255b1a\n",
      "Step 18/18 : CMD /bin/bash\n",
      " ---> Using cache\n",
      " ---> 6622701fb5dc\n",
      "Successfully built 6622701fb5dc\n",
      "Successfully tagged bert_trt:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../trt\n",
    "docker build . -f Dockerfile -t bert_trt --rm \\\n",
    "    --build-arg FROM_IMAGE_NAME=nvcr.io/nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 \\\n",
    "    --build-arg TRT_PKG_VERSION=6.0.1-1+cuda10.1 \\\n",
    "    --build-arg myuid=$(id -u) --build-arg mygid=1000 # $(id -g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Container 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 docker 실행 명령을 이용하여 BERT TensorRT engine을 build하기 위한 container를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc88034ae6e4ffed71d9ec7a6e9ef2d37cdc83bba3e6aeff7c597ed4d80a193a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: No such container: bert_trt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "\n",
    "GPU_ID=${1:-\"ALL\"}\n",
    "\n",
    "ENGINE_OUTPUT_DIR=\"outputs\"\n",
    "\n",
    "if [[ ! -e ${ENGINE_OUTPUT_DIR} ]]; then\n",
    "    mkdir -p ${ENGINE_OUTPUT_DIR}\n",
    "fi\n",
    "\n",
    "docker rm -f bert_trt\n",
    "docker run -d -ti \\\n",
    "    --name bert_trt${VERSION} \\\n",
    "    --runtime=nvidia \\\n",
    "    --shm-size=1g --ulimit memlock=1 --ulimit stack=67108864 \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=${GPU_ID} \\\n",
    "    -v $(pwd)/outputs:/workspace/outputs \\\n",
    "    -v $(pwd)/results/models:/workspace/models \\\n",
    "    -v $(pwd)/trt/TensorRT:/workspace/TensorRT \\\n",
    "    bert_trt bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                  PORTS               NAMES\r\n",
      "fc88034ae6e4        bert_trt            \"bash\"              2 seconds ago       Up Less than a second                       bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build TensorRT Plugin Layer library and download pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT TensorRT engine을 build하기 위해 필요한 plugin의 라이브러리와 예제를 위해 pretrained-weight를 ngc로부터 다운로드 받습니다.\n",
    "\n",
    "#### 1. Plugin build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TensorRT plugins for BERT\r\n",
      "-- The CXX compiler identification is GNU 7.4.0\r\n",
      "-- The CUDA compiler identification is NVIDIA 10.1.243\r\n",
      "-- Check for working CXX compiler: /usr/bin/c++\r\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n",
      "-- Detecting CXX compiler ABI info\r\n",
      "-- Detecting CXX compiler ABI info - done\r\n",
      "-- Detecting CXX compile features\r\n",
      "-- Detecting CXX compile features - done\r\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc\r\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- works\r\n",
      "-- Detecting CUDA compiler ABI info\r\n",
      "-- Detecting CUDA compiler ABI info - done\r\n",
      "-- Configuring done\r\n",
      "-- Generating done\r\n",
      "-- Build files have been written to: /workspace/TensorRT/demo/BERT/build\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target common\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object CMakeFiles/common.dir/workspace/TensorRT/samples/common/logger.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library libcommon.so\u001b[0m\r\n",
      "[ 15%] Built target common\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target bert_plugins\u001b[0m\r\n",
      "[ 23%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/embLayerNormPlugin.cu.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/geluPlugin.cu.o\u001b[0m\r\n",
      "[ 38%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/qkvToContextPlugin.cu.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/skipLayerNormPlugin.cu.o\u001b[0m\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/bert_plugins.dir/cmake_device_link.o\u001b[0m\r\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CUDA shared library libbert_plugins.so\u001b[0m\r\n",
      "[ 61%] Built target bert_plugins\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target sample_bert\u001b[0m\r\n",
      "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/bert/driver.cpp.o\u001b[0m\r\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/bert/bert.cpp.o\u001b[0m\r\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/util/dataUtils.cpp.o\u001b[0m\r\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/sampleBERT.cpp.o\u001b[0m\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable sample_bert\u001b[0m\r\n",
      "[100%] Built target sample_bert\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [[ -e ../trt/TensorRT/demo/BERT/build ]]; then\n",
    "    rm -rf ../trt/TensorRT/demo/BERT/build\n",
    "fi\n",
    "docker exec -t bert_trt bash /workspace/TensorRT/demo/BERT/python/build_plugins.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행결과 아래 경로에 build 한 결과가 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMakeCache.txt\tMakefile\t     libbert_plugins.so  sample_bert\r\n",
      "CMakeFiles\tcmake_install.cmake  libcommon.so\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../trt/TensorRT/demo/BERT/build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-trained weight download\n",
    "NGC로부터 pre-trained weight를 다운로드 받습니다. NGC에서는 다음의 조건에 대한 pretrained weight를 제공하므로 이 중에 선택하여 사용할 수 있습니다.\n",
    "\n",
    "| | options |\n",
    "|:---:|:---:|\n",
    "| model | large, base |\n",
    "| precision | fp32, fp16 |\n",
    "| seq. length | 128, 384 |\n",
    "\n",
    "물론 독자적으로 학습하신 weight (ckpt)를 사용하실 수도 있습니다.\n",
    "\n",
    "이 예제에서는 bert-large, fp16, seq-len 128 을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'large' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'large'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "if [[ ! -d ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/ ]]; then\n",
    "    docker exec -t bert_trt bash /workspace/TensorRT/demo/BERT/python/download_fine-tuned_model.sh ${MODEL} ${FT_PRECISION} ${SEQ_LEN}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGC에서 다운로드 받은 pretrained weight는 ```/results/models/fine-tuned/```에 저장이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json\n",
      "model.ckpt-8144.data-00000-of-00001\n",
      "model.ckpt-8144.index\n",
      "model.ckpt-8144.meta\n",
      "tf_bert_squad_1n_fp16_gbs32.190523100044.log\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'large' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'large'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "ls ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 BERT TensorRT precision을 build할 것입니다. 이전에 NGC model repository에서 다운로드 받은 pretrained weight와 동일한 조건으로 engine을 build하되 batch size를 지정해 줘야 합니다. 현재 버전에서는 Batch size 1만을 지원하므로, 여기서는 batch size를 1로만 지정해서 테스트 하도록 하겠습니다. 실행시간이 Telsa V100 기준으로 **20분** 가량 소요되므로 신중히 실행하시기 바랍니다.\n",
    "\n",
    "참고로 별도의 terminal 창을 열어서 ```watch -n1 nvidia-smi```를 이용해서 TensorRT가 engine을 Build하면서 GPU를 점유하는 것을 보실 수 있습니다. 이 과정에서 GPU를 사용하는 이유는 engine을 build하는 과정에서 target GPU의 성능을 측정하여 적절한 GPU Kernel의 구성을 TensorRT가 찾기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: Using configuration file: /workspace/models/fine-tuned/bert_tf_v2_large_fp16_128_v2/bert_config.json\n",
      "[TensorRT] INFO: Found 394 entries in weight map\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Saving Engine to /workspace/outputs/bert_large_128.engine\n",
      "[TensorRT] INFO: Done.\n",
      "CPU times: user 13.4 ms, sys: 8.51 ms, total: 21.9 ms\n",
      "Wall time: 14min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash -s 'large' 'fp16' '128' '1' \n",
    "\n",
    "MODEL=${1:-'large'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_builder.py \\\n",
    "            -m /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/model.ckpt-8144 \\\n",
    "            -c /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2 \\\n",
    "            -o /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine \\\n",
    "            -s ${SEQ_LEN} -b ${BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 Script의 실행결과 아래 경로에 engine 파일이 생성된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_large_128.engine\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti bert_trt ls /workspace/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 build 한 TensorRT engine을 이용해서 inference를 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\r\n",
      "WARNING:tensorflow:From /workspace/TensorRT/demo/BERT/python/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "\r\n",
      "Question: What is TensorRT?\r\n",
      "\r\n",
      "Running Inference...\r\n",
      "------------------------\r\n",
      "Running inference in 233.796 Sentences/Sec\r\n",
      "------------------------\r\n",
      "Processing output 0 in batch\r\n",
      "Answer: 'a high performance deep learning inference platform'\r\n",
      "With probability: 46.304\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'large' 'fp16' '128' '1'\n",
    "MODEL=${1:-'large'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_inference.py \\\n",
    "            -e /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine -s ${SEQ_LEN} \\\n",
    "            -p \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\" \\\n",
    "            -q \"What is TensorRT?\" \\\n",
    "            -v /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Closing the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f bert_trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT inferencing platform with TRTIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pulling TRTIS docker image\n",
    "\n",
    "TensorRT Inference Server는 새로운 Build 없이 Serving을 할 수 있는 장점이 있습니다. 우선 원활한 예제의 실행을 위해 사용할 이미지를 다음 명령을 이용하여 pull 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.10-py3: Pulling from nvidia/tensorrtserver\n",
      "5667fdb72017: Already exists\n",
      "d83811f270d5: Already exists\n",
      "ee671aafb583: Already exists\n",
      "7fc152dfb3a6: Already exists\n",
      "dbc57626691b: Already exists\n",
      "e20092842144: Already exists\n",
      "d64c76da70d5: Already exists\n",
      "429f0b34bf97: Already exists\n",
      "39d853a0098c: Already exists\n",
      "dc9dfc23df66: Already exists\n",
      "1a32524cb863: Already exists\n",
      "d3d394313ced: Already exists\n",
      "857b6050fd78: Already exists\n",
      "3a51649b9b50: Already exists\n",
      "885e286ed6cc: Already exists\n",
      "62be33d17790: Already exists\n",
      "6a7d05a28b83: Already exists\n",
      "11ff4c1b1e9b: Already exists\n",
      "252fb308c785: Already exists\n",
      "4749ee710260: Already exists\n",
      "47668c0cb079: Already exists\n",
      "4f9ec6b1521d: Already exists\n",
      "292b425b68e8: Already exists\n",
      "93e46b746825: Already exists\n",
      "d66e2a94ffdd: Pulling fs layer\n",
      "9ec0ad11e3f4: Pulling fs layer\n",
      "28efceee1d39: Pulling fs layer\n",
      "026a283c83f0: Pulling fs layer\n",
      "af0f2fe8c66a: Pulling fs layer\n",
      "ef30f655718e: Pulling fs layer\n",
      "0b20230b4afa: Pulling fs layer\n",
      "bd575020981a: Pulling fs layer\n",
      "27fab5730dad: Pulling fs layer\n",
      "d6e887bc890d: Pulling fs layer\n",
      "aad3f5aa61ff: Pulling fs layer\n",
      "865e767514bd: Pulling fs layer\n",
      "d02230245f58: Pulling fs layer\n",
      "026a283c83f0: Waiting\n",
      "8e2c5df66c6c: Pulling fs layer\n",
      "47f4926fba6a: Pulling fs layer\n",
      "0b20230b4afa: Waiting\n",
      "af0f2fe8c66a: Waiting\n",
      "615aae853aba: Pulling fs layer\n",
      "de3ab30e88da: Pulling fs layer\n",
      "bd575020981a: Waiting\n",
      "ef30f655718e: Waiting\n",
      "e5fd197ed72d: Pulling fs layer\n",
      "8e2c5df66c6c: Waiting\n",
      "27fab5730dad: Waiting\n",
      "ff3f6661e077: Pulling fs layer\n",
      "47f4926fba6a: Waiting\n",
      "d6e887bc890d: Waiting\n",
      "e5fd197ed72d: Waiting\n",
      "ff3f6661e077: Waiting\n",
      "de3ab30e88da: Waiting\n",
      "aad3f5aa61ff: Waiting\n",
      "615aae853aba: Waiting\n",
      "d02230245f58: Waiting\n",
      "865e767514bd: Waiting\n",
      "28efceee1d39: Verifying Checksum\n",
      "28efceee1d39: Download complete\n",
      "d66e2a94ffdd: Download complete\n",
      "026a283c83f0: Verifying Checksum\n",
      "026a283c83f0: Download complete\n",
      "af0f2fe8c66a: Verifying Checksum\n",
      "af0f2fe8c66a: Download complete\n",
      "ef30f655718e: Verifying Checksum\n",
      "ef30f655718e: Download complete\n",
      "0b20230b4afa: Verifying Checksum\n",
      "0b20230b4afa: Download complete\n",
      "d66e2a94ffdd: Pull complete\n",
      "9ec0ad11e3f4: Verifying Checksum\n",
      "9ec0ad11e3f4: Download complete\n",
      "bd575020981a: Verifying Checksum\n",
      "bd575020981a: Download complete\n",
      "27fab5730dad: Download complete\n",
      "aad3f5aa61ff: Verifying Checksum\n",
      "aad3f5aa61ff: Download complete\n",
      "d02230245f58: Verifying Checksum\n",
      "d02230245f58: Download complete\n",
      "8e2c5df66c6c: Download complete\n",
      "47f4926fba6a: Verifying Checksum\n",
      "47f4926fba6a: Download complete\n",
      "9ec0ad11e3f4: Pull complete\n",
      "615aae853aba: Verifying Checksum\n",
      "615aae853aba: Download complete\n",
      "28efceee1d39: Pull complete\n",
      "de3ab30e88da: Verifying Checksum\n",
      "de3ab30e88da: Download complete\n",
      "e5fd197ed72d: Download complete\n",
      "ff3f6661e077: Verifying Checksum\n",
      "ff3f6661e077: Download complete\n",
      "026a283c83f0: Pull complete\n",
      "af0f2fe8c66a: Pull complete\n",
      "ef30f655718e: Pull complete\n",
      "865e767514bd: Verifying Checksum\n",
      "865e767514bd: Download complete\n",
      "0b20230b4afa: Pull complete\n",
      "bd575020981a: Pull complete\n",
      "27fab5730dad: Pull complete\n",
      "d6e887bc890d: Verifying Checksum\n",
      "d6e887bc890d: Download complete\n",
      "d6e887bc890d: Pull complete\n",
      "aad3f5aa61ff: Pull complete\n",
      "865e767514bd: Pull complete\n",
      "d02230245f58: Pull complete\n",
      "8e2c5df66c6c: Pull complete\n",
      "47f4926fba6a: Pull complete\n",
      "615aae853aba: Pull complete\n",
      "de3ab30e88da: Pull complete\n",
      "e5fd197ed72d: Pull complete\n",
      "ff3f6661e077: Pull complete\n",
      "Digest: sha256:3d44d3a2e27b6c30ebbd693ddd38753e6c4645d9706968612296233f76be308f\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tensorrtserver:19.10-py3\n",
      "nvcr.io/nvidia/tensorrtserver:19.10-py3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker pull nvcr.io/nvidia/tensorrtserver:19.10-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting TRTIS model repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 명령들을 이용하여 TRTIS model repository를 구성합니다. 여기서 숫자 1은 model version으로 원하는 버전을 설정하실 수 있으며, 향후에 inference client 단에서 원하는 버전을 지정하여 inference가 되도록 지정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../results/trtis_models/bert_large_128_fp16\n",
    "mkdir -p ../results/trtis_models/bert_large_128_fp16/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model repository에는 model를 명시하는 ```config.pbtxt``` 파일과 TensorRT engine 파일을 ```model.plan```으로 이름을 변경하여 버전에 따라 저장을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../results/trtis_models/bert_large_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_large_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_large_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0]\n",
    "        profile: \"0\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp ../outputs/bert_large_128.engine ../results/trtis_models/bert_large_128_fp16/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model repository 구성과 함께 중요한 절차는 TensorRT를 이용하여 inference하는데 필요한 Plugin을 TRTIS에게 알려주는 것입니다. 그 이유는, TRTIS 입장에서는 TensorRT engine에서 사용하는 plugin의 정보를 사전에 알 방법이 없기 때문입니다. 이런 library는 여러개로 늘어날 수 있으므로 BERT 예제의 build directory에 두지 않고 공용으로 관리하기 좋은 별도의 공간으로 옮기도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../trt/plugins\n",
    "cp ../trt/TensorRT/demo/BERT/build/*.so ../trt/plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launch Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TensorRT inference server를 구동시킬 차례입니다. TensorFlow Model이 FP16으로 Inference 되도록 하게 하는 한편, TensorRT engine의 dependency를 위한 경로를 설정해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp16 activated!\n",
      "ac4f9ee7c730c6336bd97fefa80c4b71e9f634d68b93d124ff859ac6e8ae0f06\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"fp16\"\n",
    "\n",
    "cd ..\n",
    "\n",
    "precision=${1:-\"fp16\"}\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "   echo \"fp16 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1\n",
    "else\n",
    "   echo \"fp32 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=0\n",
    "fi\n",
    "\n",
    "# Start TRTIS server in detached state\n",
    "docker run -d --rm \\\n",
    "   --runtime=nvidia \\\n",
    "   --shm-size=1g \\\n",
    "   --ulimit memlock=-1 \\\n",
    "   --ulimit stack=67108864 \\\n",
    "   -p8000:8000 \\\n",
    "   -p8001:8001 \\\n",
    "   -p8002:8002 \\\n",
    "   --name trt_server_cont \\\n",
    "   -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "   -e TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE \\\n",
    "   -v $(pwd)/results/trtis_models:/models \\\n",
    "   -v $(pwd)/trt/plugins:/opt/tensorrtserver/lib/plugins \\\n",
    "   -e LD_PRELOAD=\"/opt/tensorrtserver/lib/plugins/libcommon.so:/opt/tensorrtserver/lib/plugins/libbert_plugins.so\" \\\n",
    "   nvcr.io/nvidia/tensorrtserver:19.10-py3 \\\n",
    "        trtserver --model-store=/models --strict-model-config=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BERT client build (optional)\n",
    "만약 BERT Training을 위해 사용한 BERT docker image가 위치한 노드와 동일한 노드라면 다음 명령은 생략하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.08-py3: Pulling from nvidia/tensorrtserver\n",
      "Digest: sha256:438b6c2ddfd095faf3453f348c8639ea5be0c28a687a604d6f691f07469076c6\n",
      "Status: Image is up to date for nvcr.io/nvidia/tensorrtserver:19.08-py3\n",
      "nvcr.io/nvidia/tensorrtserver:19.08-py3\n",
      "Sending build context to Docker daemon  1.591GB\r",
      "\r\n",
      "Step 1/19 : ARG FROM_IMAGE_NAME=nvcr.io/nvidia/tensorflow:19.08-py3\n",
      "Step 2/19 : FROM ${FROM_IMAGE_NAME}\n",
      "19.08-py3: Pulling from nvidia/tensorflow\n",
      "7413c47ba209: Already exists\n",
      "0fe7e7cbb2e8: Already exists\n",
      "1d425c982345: Already exists\n",
      "344da5c95cec: Already exists\n",
      "ae62549b429d: Already exists\n",
      "e275e0ef6c20: Already exists\n",
      "4090c4d315fe: Already exists\n",
      "00a11b299176: Already exists\n",
      "74a29ca83919: Already exists\n",
      "a1abd2d74110: Already exists\n",
      "90d7249fe09b: Already exists\n",
      "5db1b1a35ea4: Already exists\n",
      "b160969adc93: Already exists\n",
      "0179f14b1047: Already exists\n",
      "a58b5dcd3fa6: Already exists\n",
      "e7af950e37dd: Already exists\n",
      "e880be2d991d: Already exists\n",
      "b7c0ae26dc75: Already exists\n",
      "423736729fa4: Already exists\n",
      "9595d4b4fa6d: Already exists\n",
      "d18ab9b3cee4: Already exists\n",
      "d13f74634ff4: Already exists\n",
      "6465f099eaee: Already exists\n",
      "1d25a5143caf: Already exists\n",
      "b863ed055d32: Pulling fs layer\n",
      "d7c7f6dae015: Pulling fs layer\n",
      "d16fb958ffce: Pulling fs layer\n",
      "dc1d1b858f8d: Pulling fs layer\n",
      "6ba9ceffc16c: Pulling fs layer\n",
      "9736392613d1: Pulling fs layer\n",
      "379930b0d052: Pulling fs layer\n",
      "6ba9ceffc16c: Waiting\n",
      "74848a82745a: Pulling fs layer\n",
      "dc1d1b858f8d: Waiting\n",
      "45feee628410: Pulling fs layer\n",
      "379930b0d052: Waiting\n",
      "89a8956c6c35: Pulling fs layer\n",
      "9736392613d1: Waiting\n",
      "74848a82745a: Waiting\n",
      "45feee628410: Waiting\n",
      "abce4c168c8f: Pulling fs layer\n",
      "c05bf1206572: Pulling fs layer\n",
      "c275ba3d2b9d: Pulling fs layer\n",
      "0dcd07dd2212: Pulling fs layer\n",
      "c275ba3d2b9d: Waiting\n",
      "81eea7d16450: Pulling fs layer\n",
      "4ee086c2d8f9: Pulling fs layer\n",
      "0dcd07dd2212: Waiting\n",
      "81eea7d16450: Waiting\n",
      "16c72bfe563b: Pulling fs layer\n",
      "9f145ca68032: Pulling fs layer\n",
      "4ee086c2d8f9: Waiting\n",
      "434476c6b401: Pulling fs layer\n",
      "9f145ca68032: Waiting\n",
      "0d74e44a341b: Pulling fs layer\n",
      "434476c6b401: Waiting\n",
      "f7bc497efb25: Pulling fs layer\n",
      "0d74e44a341b: Waiting\n",
      "b9b73f1072c4: Pulling fs layer\n",
      "af67682a070e: Pulling fs layer\n",
      "b9b73f1072c4: Waiting\n",
      "a8816dce234d: Pulling fs layer\n",
      "9b8651f964db: Pulling fs layer\n",
      "3f2a9601ce6f: Pulling fs layer\n",
      "7fceef0e4dba: Pulling fs layer\n",
      "af67682a070e: Waiting\n",
      "3f2a9601ce6f: Waiting\n",
      "a8816dce234d: Waiting\n",
      "9b8651f964db: Waiting\n",
      "2342a54620ff: Pulling fs layer\n",
      "7fceef0e4dba: Waiting\n",
      "1def91980a11: Pulling fs layer\n",
      "2342a54620ff: Waiting\n",
      "e47186c04458: Pulling fs layer\n",
      "f00e669ae754: Pulling fs layer\n",
      "b9f1e77afb25: Pulling fs layer\n",
      "f12319182589: Pulling fs layer\n",
      "5578439cd807: Pulling fs layer\n",
      "e47186c04458: Waiting\n",
      "0661540359cb: Pulling fs layer\n",
      "1def91980a11: Waiting\n",
      "f12319182589: Waiting\n",
      "f00e669ae754: Waiting\n",
      "e10f1743bdaf: Pulling fs layer\n",
      "5578439cd807: Waiting\n",
      "b9f1e77afb25: Waiting\n",
      "0661540359cb: Waiting\n",
      "e10f1743bdaf: Waiting\n",
      "f5721f24a795: Pulling fs layer\n",
      "504d46c2bb2a: Pulling fs layer\n",
      "f71574d9f199: Pulling fs layer\n",
      "f5721f24a795: Waiting\n",
      "951a7d7d79a0: Pulling fs layer\n",
      "f71574d9f199: Waiting\n",
      "9bfbaf26a2fa: Pulling fs layer\n",
      "951a7d7d79a0: Waiting\n",
      "169198376041: Pulling fs layer\n",
      "9bfbaf26a2fa: Waiting\n",
      "89d1c502bc27: Pulling fs layer\n",
      "89d1c502bc27: Waiting\n",
      "d7c7f6dae015: Verifying Checksum\n",
      "d7c7f6dae015: Download complete\n",
      "d16fb958ffce: Verifying Checksum\n",
      "d16fb958ffce: Download complete\n",
      "dc1d1b858f8d: Verifying Checksum\n",
      "dc1d1b858f8d: Download complete\n",
      "9736392613d1: Download complete\n",
      "6ba9ceffc16c: Verifying Checksum\n",
      "6ba9ceffc16c: Download complete\n",
      "74848a82745a: Download complete\n",
      "45feee628410: Download complete\n",
      "b863ed055d32: Verifying Checksum\n",
      "b863ed055d32: Download complete\n",
      "89a8956c6c35: Verifying Checksum\n",
      "89a8956c6c35: Download complete\n",
      "abce4c168c8f: Verifying Checksum\n",
      "abce4c168c8f: Download complete\n",
      "c05bf1206572: Verifying Checksum\n",
      "c05bf1206572: Download complete\n",
      "379930b0d052: Verifying Checksum\n",
      "379930b0d052: Download complete\n",
      "81eea7d16450: Download complete\n",
      "4ee086c2d8f9: Verifying Checksum\n",
      "4ee086c2d8f9: Download complete\n",
      "0dcd07dd2212: Verifying Checksum\n",
      "0dcd07dd2212: Download complete\n",
      "9f145ca68032: Download complete\n",
      "434476c6b401: Verifying Checksum\n",
      "434476c6b401: Download complete\n",
      "16c72bfe563b: Verifying Checksum\n",
      "16c72bfe563b: Download complete\n",
      "f7bc497efb25: Verifying Checksum\n",
      "f7bc497efb25: Download complete\n",
      "b9b73f1072c4: Verifying Checksum\n",
      "b9b73f1072c4: Download complete\n",
      "c275ba3d2b9d: Verifying Checksum\n",
      "c275ba3d2b9d: Download complete\n",
      "a8816dce234d: Download complete\n",
      "9b8651f964db: Download complete\n",
      "3f2a9601ce6f: Verifying Checksum\n",
      "3f2a9601ce6f: Download complete\n",
      "7fceef0e4dba: Verifying Checksum\n",
      "7fceef0e4dba: Download complete\n",
      "af67682a070e: Verifying Checksum\n",
      "af67682a070e: Download complete\n",
      "2342a54620ff: Download complete\n",
      "e47186c04458: Download complete\n",
      "f00e669ae754: Verifying Checksum\n",
      "f00e669ae754: Download complete\n",
      "b9f1e77afb25: Verifying Checksum\n",
      "b9f1e77afb25: Download complete\n",
      "f12319182589: Verifying Checksum\n",
      "f12319182589: Download complete\n",
      "5578439cd807: Verifying Checksum\n",
      "5578439cd807: Download complete\n",
      "0d74e44a341b: Verifying Checksum\n",
      "0d74e44a341b: Download complete\n",
      "e10f1743bdaf: Verifying Checksum\n",
      "e10f1743bdaf: Download complete\n",
      "f5721f24a795: Verifying Checksum\n",
      "f5721f24a795: Download complete\n",
      "504d46c2bb2a: Verifying Checksum\n",
      "504d46c2bb2a: Download complete\n",
      "f71574d9f199: Verifying Checksum\n",
      "f71574d9f199: Download complete\n",
      "951a7d7d79a0: Verifying Checksum\n",
      "951a7d7d79a0: Download complete\n",
      "9bfbaf26a2fa: Verifying Checksum\n",
      "9bfbaf26a2fa: Download complete\n",
      "169198376041: Verifying Checksum\n",
      "169198376041: Download complete\n",
      "89d1c502bc27: Verifying Checksum\n",
      "89d1c502bc27: Download complete\n",
      "0661540359cb: Verifying Checksum\n",
      "0661540359cb: Download complete\n",
      "b863ed055d32: Pull complete\n",
      "1def91980a11: Verifying Checksum\n",
      "d7c7f6dae015: Pull complete\n",
      "d16fb958ffce: Pull complete\n",
      "dc1d1b858f8d: Pull complete\n",
      "6ba9ceffc16c: Pull complete\n",
      "9736392613d1: Pull complete\n",
      "379930b0d052: Pull complete\n",
      "74848a82745a: Pull complete\n",
      "45feee628410: Pull complete\n",
      "89a8956c6c35: Pull complete\n",
      "abce4c168c8f: Pull complete\n",
      "c05bf1206572: Pull complete\n",
      "c275ba3d2b9d: Pull complete\n",
      "0dcd07dd2212: Pull complete\n",
      "81eea7d16450: Pull complete\n",
      "4ee086c2d8f9: Pull complete\n",
      "16c72bfe563b: Pull complete\n",
      "9f145ca68032: Pull complete\n",
      "434476c6b401: Pull complete\n",
      "0d74e44a341b: Pull complete\n",
      "f7bc497efb25: Pull complete\n",
      "b9b73f1072c4: Pull complete\n",
      "af67682a070e: Pull complete\n",
      "a8816dce234d: Pull complete\n",
      "9b8651f964db: Pull complete\n",
      "3f2a9601ce6f: Pull complete\n",
      "7fceef0e4dba: Pull complete\n",
      "2342a54620ff: Pull complete\n",
      "1def91980a11: Pull complete\n",
      "e47186c04458: Pull complete\n",
      "f00e669ae754: Pull complete\n",
      "b9f1e77afb25: Pull complete\n",
      "f12319182589: Pull complete\n",
      "5578439cd807: Pull complete\n",
      "0661540359cb: Pull complete\n",
      "e10f1743bdaf: Pull complete\n",
      "f5721f24a795: Pull complete\n",
      "504d46c2bb2a: Pull complete\n",
      "f71574d9f199: Pull complete\n",
      "951a7d7d79a0: Pull complete\n",
      "9bfbaf26a2fa: Pull complete\n",
      "169198376041: Pull complete\n",
      "89d1c502bc27: Pull complete\n",
      "Digest: sha256:64e296668d398a106f64bd840772ffb63372148b8c1170b152e7e577013661c9\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tensorflow:19.08-py3\n",
      " ---> be978d32a5c3\n",
      "Step 3/19 : RUN apt-get update && apt-get install -y pbzip2 pv bzip2 libcurl4 curl\n",
      " ---> Running in 908f38c8f900\n",
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [804 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [23.7 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [780 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [6779 B]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1077 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [37.4 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [10.8 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1335 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4241 B]\n",
      "Fetched 17.5 MB in 3s (5273 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "bzip2 is already the newest version (1.0.6-8.1ubuntu0.2).\n",
      "Suggested packages:\n",
      "  doc-base\n",
      "The following NEW packages will be installed:\n",
      "  pbzip2 pv\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4\n",
      "2 upgraded, 2 newly installed, 0 to remove and 51 not upgraded.\n",
      "Need to get 458 kB of archives.\n",
      "After this operation, 220 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 curl amd64 7.58.0-2ubuntu3.8 [159 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcurl4 amd64 7.58.0-2ubuntu3.8 [214 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pbzip2 amd64 1.1.9-1build1 [37.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 pv amd64 1.6.6-1 [48.3 kB]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 458 kB in 1s (407 kB/s)\n",
      "(Reading database ... \r",
      "(Reading database ... 5%\r",
      "(Reading database ... 10%\r",
      "(Reading database ... 15%\r",
      "(Reading database ... 20%\r",
      "(Reading database ... 25%\r",
      "(Reading database ... 30%\r",
      "(Reading database ... 35%\r",
      "(Reading database ... 40%\r",
      "(Reading database ... 45%\r",
      "(Reading database ... 50%\r",
      "(Reading database ... 55%\r",
      "(Reading database ... 60%\r",
      "(Reading database ... 65%\r",
      "(Reading database ... 70%\r",
      "(Reading database ... 75%\r",
      "(Reading database ... 80%\r",
      "(Reading database ... 85%\r",
      "(Reading database ... 90%\r",
      "(Reading database ... 95%\r",
      "(Reading database ... 100%\r",
      "(Reading database ... 34622 files and directories currently installed.)\r\n",
      "Preparing to unpack .../curl_7.58.0-2ubuntu3.8_amd64.deb ...\r\n",
      "Unpacking curl (7.58.0-2ubuntu3.8) over (7.58.0-2ubuntu3.7) ...\r\n",
      "Preparing to unpack .../libcurl4_7.58.0-2ubuntu3.8_amd64.deb ...\r\n",
      "Unpacking libcurl4:amd64 (7.58.0-2ubuntu3.8) over (7.58.0-2ubuntu3.7) ...\r\n",
      "Selecting previously unselected package pbzip2.\r\n",
      "Preparing to unpack .../pbzip2_1.1.9-1build1_amd64.deb ...\r\n",
      "Unpacking pbzip2 (1.1.9-1build1) ...\r\n",
      "Selecting previously unselected package pv.\r\n",
      "Preparing to unpack .../archives/pv_1.6.6-1_amd64.deb ...\r\n",
      "Unpacking pv (1.6.6-1) ...\r\n",
      "Setting up pv (1.6.6-1) ...\r\n",
      "Setting up libcurl4:amd64 (7.58.0-2ubuntu3.8) ...\r\n",
      "Setting up pbzip2 (1.1.9-1build1) ...\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
      "Setting up curl (7.58.0-2ubuntu3.8) ...\r\n",
      "Removing intermediate container 908f38c8f900\n",
      " ---> 28aacc4d9d1e\n",
      "Step 4/19 : RUN pip install toposort networkx pytest nltk tqdm html2text progressbar\n",
      " ---> Running in 5f28418ab28a\n",
      "Collecting toposort\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Collecting networkx\n",
      "  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "Collecting pytest\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/94/29b5a09edc970a2f24742e6186dfe497cd6c778e60e54693215a36852613/pytest-5.3.3-py3-none-any.whl (235kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.35.0)\n",
      "Collecting html2text\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
      "Collecting progressbar\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.0)\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\" (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/d7/31/74dcb59a601b95fce3b0334e8fc9db758f78e43075f22aeb3677dfb19f4c/importlib_metadata-1.4.0-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "Collecting packaging (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/d8/5b/3098db49a61ccc8583ffead6aedc226f08ff56dc03106b6ec54451e27a30/packaging-20.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (19.1.0)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest) (0.1.7)\n",
      "Collecting more-itertools>=4.0.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/e2/3206a70758a21f9878fcf9478282bb68fbc66a5564718f9ed724c3f2bb52/more_itertools-8.1.0-py3-none-any.whl (41kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/50/cc72c5bcd48f6e98219fc4a88a5227e9e28b81637a99c49feba1d51f4d50/zipp-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (2.4.2)\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py): started\n",
      "  Building wheel for progressbar (setup.py): finished with status 'done'\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=943cfbe2de7d1ad93adb6d64416537b0ff75460b66be0718ea1ce5e9ec76530d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
      "Successfully built progressbar\n",
      "Installing collected packages: toposort, networkx, more-itertools, zipp, importlib-metadata, py, packaging, pluggy, pytest, html2text, progressbar\n",
      "Successfully installed html2text-2020.1.16 importlib-metadata-1.4.0 more-itertools-8.1.0 networkx-2.4 packaging-20.0 pluggy-0.13.1 progressbar-2.5 py-1.8.1 pytest-5.3.3 toposort-1.5 zipp-1.0.0\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 5f28418ab28a\n",
      " ---> 1b6de7f3f482\n",
      "Step 5/19 : WORKDIR /workspace\n",
      " ---> Running in 9e32ed351d12\n",
      "Removing intermediate container 9e32ed351d12\n",
      " ---> b1df213ba501\n",
      "Step 6/19 : RUN git clone https://github.com/openai/gradient-checkpointing.git\n",
      " ---> Running in 1562dcdc32ee\n",
      "\u001b[91mCloning into 'gradient-checkpointing'...\n",
      "\u001b[0mRemoving intermediate container 1562dcdc32ee\n",
      " ---> dce225321875\n",
      "Step 7/19 : RUN git clone https://github.com/attardi/wikiextractor.git\n",
      " ---> Running in 9aecd49f7551\n",
      "\u001b[91mCloning into 'wikiextractor'...\n",
      "\u001b[0mRemoving intermediate container 9aecd49f7551\n",
      " ---> 74dc0612ae70\n",
      "Step 8/19 : RUN git clone https://github.com/soskek/bookcorpus.git\n",
      " ---> Running in 98d2cc4ad866\n",
      "\u001b[91mCloning into 'bookcorpus'...\n",
      "\u001b[0mRemoving intermediate container 98d2cc4ad866\n",
      " ---> d9b801dec147\n",
      "Step 9/19 : RUN git clone https://github.com/titipata/pubmed_parser\n",
      " ---> Running in 8639cb06211f\n",
      "\u001b[91mCloning into 'pubmed_parser'...\n",
      "\u001b[0mRemoving intermediate container 8639cb06211f\n",
      " ---> 084827a38920\n",
      "Step 10/19 : RUN pip3 install /workspace/pubmed_parser\n",
      " ---> Running in feaf4368c703\n",
      "Processing ./pubmed_parser\n",
      "Collecting lxml (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/30/affd16b77edf9537f5be051905f33527021e20d563d013e8c42c7fd01949/lxml-4.4.2-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "Collecting unidecode (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "Collecting requests (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl (125kB)\n",
      "Collecting idna<2.9,>=2.5 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Building wheels for collected packages: pubmed-parser\n",
      "  Building wheel for pubmed-parser (setup.py): started\n",
      "  Building wheel for pubmed-parser (setup.py): finished with status 'done'\n",
      "  Created wheel for pubmed-parser: filename=pubmed_parser-0.2.1-cp36-none-any.whl size=15037 sha256=87c913b69944643d669e572b508ca8d399239dcb3822bf1ad164e31f864f7839\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7s719_dz/wheels/70/0e/94/406257b015fc1ba650bee2b5e3fd979b281504f67008d482f3\n",
      "Successfully built pubmed-parser\n",
      "Installing collected packages: lxml, unidecode, chardet, urllib3, idna, certifi, requests, pubmed-parser\n",
      "Successfully installed certifi-2019.11.28 chardet-3.0.4 idna-2.8 lxml-4.4.2 pubmed-parser-0.2.1 requests-2.22.0 unidecode-1.1.1 urllib3-1.25.7\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container feaf4368c703\n",
      " ---> 62765119f676\n",
      "Step 11/19 : ARG TRTIS_CLIENTS_URL=https://github.com/NVIDIA/tensorrt-inference-server/releases/download/v1.5.0/v1.5.0_ubuntu1804.clients.tar.gz\n",
      " ---> Running in 23a06726a892\n",
      "Removing intermediate container 23a06726a892\n",
      " ---> f470a5c08c9b\n",
      "Step 12/19 : RUN mkdir -p /workspace/install     && curl -L ${TRTIS_CLIENTS_URL} | tar xvz -C /workspace/install\n",
      " ---> Running in 521ee0a25d16\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0    \u001b[0m\u001b[91m 0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\u001b[91m\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\u001b[91m\r",
      "100   623    0   623    0     0   2396      0 --:--:-- --:--:-- --:--:--  2386\n",
      "\u001b[0mbin/\n",
      "bin/ensemble_image_client\n",
      "bin/simple_client\n",
      "bin/simple_sequence_client\n",
      "bin/image_client\n",
      "bin/perf_client\n",
      "\u001b[91m\r",
      "  4 4081k    4  168k    0     0   147k\u001b[0m\u001b[91m      0 \u001b[0m\u001b[91m 0:00:27  0:00:01  0:00:\u001b[0m\u001b[91m26  147k\u001b[0mbin/simple_string_client\n",
      "bin/simple_callback_client\n",
      "include/\n",
      "include/model_config.pb.h\n",
      "include/request_status.pb.h\n",
      "include/api.pb.h\n",
      "include/request_http.h\n",
      "include/request.h\n",
      "include/request_grpc.h\n",
      "include/server_status.pb.h\n",
      "lib/\n",
      "lib/librequest.so\n",
      "\u001b[91m\r",
      " 16 4081k   16  678k    0     0   315k      0  0:00:12  0:00:02  0:00:10  504k\u001b[0m\u001b[91m\r",
      " 32 4081k   32 1308k    0     0   413k      0  0:00:09  0:00:03  0:00:06  564k\u001b[0m\u001b[91m\r",
      " 51 4081k   51 2107k    0     0   507k      0  0:00:08  0:00:04  0:00:04  644k\u001b[0mpython/\n",
      "python/simple_string_client.py\n",
      "python/simple_callback_client.py\n",
      "python/simple_sequence_client.py\n",
      "python/grpc_image_client.py\n",
      "python/tensorrtserver-1.5.0-py2.py3-none-linux_x86_64.whl\n",
      "\u001b[91m\r",
      " 75 4081k   75 3076k    0     0   595k      0  0:00:06  0:00:05  0:00:01  723k\u001b[0m\u001b[91m\r",
      "100 4081k  100 4081k    0     0   673k      0  0:00:06  0:00:06 --:--:--  795k\n",
      "\u001b[0mpython/simple_client.py\n",
      "python/image_client.py\n",
      "python/ensemble_image_client.py\n",
      "Removing intermediate container 521ee0a25d16\n",
      " ---> 9f7c872bf0d1\n",
      "Step 13/19 : RUN pip install /workspace/install/python/tensorrtserver*.whl\n",
      " ---> Running in f087dfc2af47\n",
      "Processing ./install/python/tensorrtserver-1.5.0-py2.py3-none-linux_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (1.14.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (0.17.1)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (1.22.0)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (3.9.1)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio->tensorrtserver==1.5.0) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.5.0->tensorrtserver==1.5.0) (41.0.1)\n",
      "Installing collected packages: tensorrtserver\n",
      "Successfully installed tensorrtserver-1.5.0\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container f087dfc2af47\n",
      " ---> 1b46d0616eb3\n",
      "Step 14/19 : WORKDIR /workspace/bert\n",
      " ---> Running in a60fae5ed706\n",
      "Removing intermediate container a60fae5ed706\n",
      " ---> 008c730cc11e\n",
      "Step 15/19 : COPY . .\n",
      " ---> 1fa4cad6da76\n",
      "Step 16/19 : ENV PYTHONPATH /workspace/bert\n",
      " ---> Running in 9198294bc0fc\n",
      "Removing intermediate container 9198294bc0fc\n",
      " ---> e6ac0f88110c\n",
      "Step 17/19 : ENV BERT_PREP_WORKING_DIR /workspace/bert/data\n",
      " ---> Running in 84910270f7c6\n",
      "Removing intermediate container 84910270f7c6\n",
      " ---> f88c4340f529\n",
      "Step 18/19 : ENV PATH //workspace/install/bin:${PATH}\n",
      " ---> Running in 56ae77556f63\n",
      "Removing intermediate container 56ae77556f63\n",
      " ---> 75d137b142bd\n",
      "Step 19/19 : ENV LD_LIBRARY_PATH /workspace/install/lib:${LD_LIBRARY_PATH}\n",
      " ---> Running in b32c070cbbf7\n",
      "Removing intermediate container b32c070cbbf7\n",
      " ---> ca7c2fb6762d\n",
      "Successfully built ca7c2fb6762d\n",
      "Successfully tagged bert:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./scripts/docker/build.sh: line 7: cd: tensorrt-inference-server: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd ..\n",
    "bash ./scripts/docker/build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BERT TRTIS performance (1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      "TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "NOTE: Detected MOFED driver 4.6-1.0.1; version automatically updated.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 500 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 235 infer/sec. Avg latency: 4244 usec (std 78 usec)\r\n",
      "  Pass [2] throughput: 233 infer/sec. Avg latency: 4281 usec (std 62 usec)\r\n",
      "  Pass [3] throughput: 230 infer/sec. Avg latency: 4330 usec (std 85 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 692\r\n",
      "    Throughput: 230 infer/sec\r\n",
      "    Avg latency: 4330 usec (standard deviation 85 usec)\r\n",
      "    p50 latency: 4314 usec\r\n",
      "    p90 latency: 4469 usec\r\n",
      "    p95 latency: 4505 usec\r\n",
      "    p99 latency: 4561 usec\r\n",
      "    Avg gRPC time: 4289 usec (marshal 4 usec + response wait 4277 usec + unmarshal 8 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 829\r\n",
      "    Avg request latency: 3910 usec (overhead 8 usec + queue 38 usec + compute 3864 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 257 infer/sec. Avg latency: 7746 usec (std 91 usec)\r\n",
      "  Pass [2] throughput: 258 infer/sec. Avg latency: 7733 usec (std 93 usec)\r\n",
      "  Pass [3] throughput: 257 infer/sec. Avg latency: 7754 usec (std 109 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 773\r\n",
      "    Throughput: 257 infer/sec\r\n",
      "    Avg latency: 7754 usec (standard deviation 109 usec)\r\n",
      "    p50 latency: 7731 usec\r\n",
      "    p90 latency: 7824 usec\r\n",
      "    p95 latency: 7926 usec\r\n",
      "    p99 latency: 8049 usec\r\n",
      "    Avg gRPC time: 7708 usec (marshal 4 usec + response wait 7696 usec + unmarshal 8 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 928\r\n",
      "    Avg request latency: 7340 usec (overhead 10 usec + queue 3500 usec + compute 3830 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 257 infer/sec. Avg latency: 11666 usec (std 137 usec)\r\n",
      "  Pass [2] throughput: 257 infer/sec. Avg latency: 11648 usec (std 86 usec)\r\n",
      "  Pass [3] throughput: 258 infer/sec. Avg latency: 11603 usec (std 104 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 776\r\n",
      "    Throughput: 258 infer/sec\r\n",
      "    Avg latency: 11603 usec (standard deviation 104 usec)\r\n",
      "    p50 latency: 11588 usec\r\n",
      "    p90 latency: 11650 usec\r\n",
      "    p95 latency: 11687 usec\r\n",
      "    p99 latency: 12234 usec\r\n",
      "    Avg gRPC time: 11569 usec (marshal 4 usec + response wait 11557 usec + unmarshal 8 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 930\r\n",
      "    Avg request latency: 11145 usec (overhead 11 usec + queue 7317 usec + compute 3817 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 258 infer/sec. Avg latency: 15489 usec (std 63 usec)\r\n",
      "  Pass [2] throughput: 258 infer/sec. Avg latency: 15467 usec (std 152 usec)\r\n",
      "  Pass [3] throughput: 259 infer/sec. Avg latency: 15403 usec (std 52 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 778\r\n",
      "    Throughput: 259 infer/sec\r\n",
      "    Avg latency: 15403 usec (standard deviation 52 usec)\r\n",
      "    p50 latency: 15397 usec\r\n",
      "    p90 latency: 15458 usec\r\n",
      "    p95 latency: 15485 usec\r\n",
      "    p99 latency: 15597 usec\r\n",
      "    Avg gRPC time: 15361 usec (marshal 4 usec + response wait 15349 usec + unmarshal 8 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 934\r\n",
      "    Avg request latency: 14918 usec (overhead 11 usec + queue 11104 usec + compute 3803 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 259 infer/sec. Avg latency: 19286 usec (std 110 usec)\r\n",
      "  Pass [2] throughput: 259 infer/sec. Avg latency: 19278 usec (std 113 usec)\r\n",
      "  Pass [3] throughput: 259 infer/sec. Avg latency: 19266 usec (std 77 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 778\r\n",
      "    Throughput: 259 infer/sec\r\n",
      "    Avg latency: 19266 usec (standard deviation 77 usec)\r\n",
      "    p50 latency: 19248 usec\r\n",
      "    p90 latency: 19348 usec\r\n",
      "    p95 latency: 19419 usec\r\n",
      "    p99 latency: 19587 usec\r\n",
      "    Avg gRPC time: 19236 usec (marshal 4 usec + response wait 19226 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 934\r\n",
      "    Avg request latency: 18813 usec (overhead 10 usec + queue 14995 usec + compute 3808 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 259 infer/sec. Avg latency: 23127 usec (std 73 usec)\r\n",
      "  Pass [2] throughput: 259 infer/sec. Avg latency: 23131 usec (std 87 usec)\r\n",
      "  Pass [3] throughput: 259 infer/sec. Avg latency: 23130 usec (std 105 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 778\r\n",
      "    Throughput: 259 infer/sec\r\n",
      "    Avg latency: 23130 usec (standard deviation 105 usec)\r\n",
      "    p50 latency: 23100 usec\r\n",
      "    p90 latency: 23237 usec\r\n",
      "    p95 latency: 23363 usec\r\n",
      "    p99 latency: 23551 usec\r\n",
      "    Avg gRPC time: 23087 usec (marshal 4 usec + response wait 23076 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 934\r\n",
      "    Avg request latency: 22649 usec (overhead 10 usec + queue 18833 usec + compute 3806 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 259 infer/sec. Avg latency: 26983 usec (std 151 usec)\r\n",
      "  Pass [2] throughput: 259 infer/sec. Avg latency: 26992 usec (std 156 usec)\r\n",
      "  Pass [3] throughput: 259 infer/sec. Avg latency: 26963 usec (std 74 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 778\r\n",
      "    Throughput: 259 infer/sec\r\n",
      "    Avg latency: 26963 usec (standard deviation 74 usec)\r\n",
      "    p50 latency: 26944 usec\r\n",
      "    p90 latency: 27063 usec\r\n",
      "    p95 latency: 27120 usec\r\n",
      "    p99 latency: 27221 usec\r\n",
      "    Avg gRPC time: 26924 usec (marshal 4 usec + response wait 26913 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 935\r\n",
      "    Avg request latency: 26520 usec (overhead 8 usec + queue 22707 usec + compute 3805 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 256 infer/sec. Avg latency: 31173 usec (std 148 usec)\r\n",
      "  Pass [2] throughput: 257 infer/sec. Avg latency: 31094 usec (std 128 usec)\r\n",
      "  Pass [3] throughput: 257 infer/sec. Avg latency: 31018 usec (std 108 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 773\r\n",
      "    Throughput: 257 infer/sec\r\n",
      "    Avg latency: 31018 usec (standard deviation 108 usec)\r\n",
      "    p50 latency: 30983 usec\r\n",
      "    p90 latency: 31149 usec\r\n",
      "    p95 latency: 31282 usec\r\n",
      "    p99 latency: 31435 usec\r\n",
      "    Avg gRPC time: 30969 usec (marshal 4 usec + response wait 30959 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 928\r\n",
      "    Avg request latency: 30606 usec (overhead 8 usec + queue 26770 usec + compute 3828 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 258 infer/sec. Avg latency: 34884 usec (std 116 usec)\r\n",
      "  Pass [2] throughput: 258 infer/sec. Avg latency: 34868 usec (std 80 usec)\r\n",
      "  Pass [3] throughput: 257 infer/sec. Avg latency: 34926 usec (std 134 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 773\r\n",
      "    Throughput: 257 infer/sec\r\n",
      "    Avg latency: 34926 usec (standard deviation 134 usec)\r\n",
      "    p50 latency: 34871 usec\r\n",
      "    p90 latency: 35115 usec\r\n",
      "    p95 latency: 35192 usec\r\n",
      "    p99 latency: 35323 usec\r\n",
      "    Avg gRPC time: 34897 usec (marshal 3 usec + response wait 34888 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 927\r\n",
      "    Avg request latency: 34531 usec (overhead 8 usec + queue 30690 usec + compute 3833 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 257 infer/sec. Avg latency: 38891 usec (std 102 usec)\r\n",
      "  Pass [2] throughput: 256 infer/sec. Avg latency: 38918 usec (std 108 usec)\r\n",
      "  Pass [3] throughput: 257 infer/sec. Avg latency: 38903 usec (std 110 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 771\r\n",
      "    Throughput: 257 infer/sec\r\n",
      "    Avg latency: 38903 usec (standard deviation 110 usec)\r\n",
      "    p50 latency: 38866 usec\r\n",
      "    p90 latency: 39090 usec\r\n",
      "    p95 latency: 39175 usec\r\n",
      "    p99 latency: 39304 usec\r\n",
      "    Avg gRPC time: 38863 usec (marshal 3 usec + response wait 38854 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 925\r\n",
      "    Avg request latency: 38498 usec (overhead 7 usec + queue 34649 usec + compute 3842 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 230 infer/sec, latency 4330 usec\r\n",
      "Concurrency: 2, 257 infer/sec, latency 7754 usec\r\n",
      "Concurrency: 3, 258 infer/sec, latency 11603 usec\r\n",
      "Concurrency: 4, 259 infer/sec, latency 15403 usec\r\n",
      "Concurrency: 5, 259 infer/sec, latency 19266 usec\r\n",
      "Concurrency: 6, 259 infer/sec, latency 23130 usec\r\n",
      "Concurrency: 7, 259 infer/sec, latency 26963 usec\r\n",
      "Concurrency: 8, 257 infer/sec, latency 31018 usec\r\n",
      "Concurrency: 9, 257 infer/sec, latency 34926 usec\r\n",
      "Concurrency: 10, 257 infer/sec, latency 38903 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"large\" \"128\" \"fp16\" \"1\" \"1\" \"500\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"large\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model reconfiguration & updated inference performance - 2 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU의 구성을 바꾸기 위해 model repository에 있는 GPU 정보를 새롭게 업데이트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../results/trtis_models/bert_large_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_large_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_large_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0, 1, 2, 3]\n",
    "        profile: \"0\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 TensorRT Inference Server에서는 이를 감지하여 새로 GPU instance를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      "TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 시점에서 nvidia-smi 등을 통해서 GPU 메모리 사용량을 보시면, 4개의 GPU에 메모리 사용량이 늘어난 것을 보실 수 있습니다.\n",
    "\n",
    "이제 테스트를 해볼 차례입니다. 다만 여기서는 Client 단의 max latency를 기존의 500에서 5000으로 조정하도록 하겠습니다. (이유 파악중..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "NOTE: Detected MOFED driver 4.6-1.0.1; version automatically updated.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 5000 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 235 infer/sec. Avg latency: 4249 usec (std 52 usec)\r\n",
      "  Pass [2] throughput: 235 infer/sec. Avg latency: 4245 usec (std 107 usec)\r\n",
      "  Pass [3] throughput: 232 infer/sec. Avg latency: 4307 usec (std 119 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 696\r\n",
      "    Throughput: 232 infer/sec\r\n",
      "    Avg latency: 4307 usec (standard deviation 119 usec)\r\n",
      "    p50 latency: 4291 usec\r\n",
      "    p90 latency: 4435 usec\r\n",
      "    p95 latency: 4499 usec\r\n",
      "    p99 latency: 4657 usec\r\n",
      "    Avg gRPC time: 4254 usec (marshal 4 usec + response wait 4243 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 836\r\n",
      "    Avg request latency: 3909 usec (overhead 10 usec + queue 44 usec + compute 3855 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 461 infer/sec. Avg latency: 4324 usec (std 116 usec)\r\n",
      "  Pass [2] throughput: 461 infer/sec. Avg latency: 4330 usec (std 94 usec)\r\n",
      "  Pass [3] throughput: 460 infer/sec. Avg latency: 4334 usec (std 96 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1382\r\n",
      "    Throughput: 460 infer/sec\r\n",
      "    Avg latency: 4334 usec (standard deviation 96 usec)\r\n",
      "    p50 latency: 4333 usec\r\n",
      "    p90 latency: 4451 usec\r\n",
      "    p95 latency: 4477 usec\r\n",
      "    p99 latency: 4562 usec\r\n",
      "    Avg gRPC time: 4301 usec (marshal 5 usec + response wait 4289 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1656\r\n",
      "    Avg request latency: 3908 usec (overhead 10 usec + queue 42 usec + compute 3856 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 686 infer/sec. Avg latency: 4360 usec (std 106 usec)\r\n",
      "  Pass [2] throughput: 682 infer/sec. Avg latency: 4390 usec (std 105 usec)\r\n",
      "  Pass [3] throughput: 688 infer/sec. Avg latency: 4356 usec (std 106 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2064\r\n",
      "    Throughput: 688 infer/sec\r\n",
      "    Avg latency: 4356 usec (standard deviation 106 usec)\r\n",
      "    p50 latency: 4343 usec\r\n",
      "    p90 latency: 4485 usec\r\n",
      "    p95 latency: 4539 usec\r\n",
      "    p99 latency: 4637 usec\r\n",
      "    Avg gRPC time: 4318 usec (marshal 4 usec + response wait 4307 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 2475\r\n",
      "    Avg request latency: 3927 usec (overhead 12 usec + queue 39 usec + compute 3876 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 909 infer/sec. Avg latency: 4395 usec (std 104 usec)\r\n",
      "  Pass [2] throughput: 905 infer/sec. Avg latency: 4410 usec (std 154 usec)\r\n",
      "  Pass [3] throughput: 904 infer/sec. Avg latency: 4416 usec (std 119 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2712\r\n",
      "    Throughput: 904 infer/sec\r\n",
      "    Avg latency: 4416 usec (standard deviation 119 usec)\r\n",
      "    p50 latency: 4403 usec\r\n",
      "    p90 latency: 4575 usec\r\n",
      "    p95 latency: 4631 usec\r\n",
      "    p99 latency: 4722 usec\r\n",
      "    Avg gRPC time: 4370 usec (marshal 4 usec + response wait 4359 usec + unmarshal 7 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3254\r\n",
      "    Avg request latency: 3954 usec (overhead 11 usec + queue 47 usec + compute 3896 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 1014 infer/sec. Avg latency: 4922 usec (std 824 usec)\r\n",
      "  Pass [2] throughput: 1013 infer/sec. Avg latency: 4926 usec (std 783 usec)\r\n",
      "  Pass [3] throughput: 1018 infer/sec. Avg latency: 4904 usec (std 794 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 3054\r\n",
      "    Throughput: 1018 infer/sec\r\n",
      "    Avg latency: 4904 usec (standard deviation 794 usec)\r\n",
      "    p50 latency: 4463 usec\r\n",
      "    p90 latency: 6287 usec\r\n",
      "    p95 latency: 6386 usec\r\n",
      "    p99 latency: 6573 usec\r\n",
      "    Avg gRPC time: 4869 usec (marshal 3 usec + response wait 4860 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3664\r\n",
      "    Avg request latency: 4556 usec (overhead 10 usec + queue 676 usec + compute 3870 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 1009 infer/sec. Avg latency: 5936 usec (std 895 usec)\r\n",
      "  Pass [2] throughput: 1007 infer/sec. Avg latency: 5954 usec (std 850 usec)\r\n",
      "  Pass [3] throughput: 999 infer/sec. Avg latency: 5998 usec (std 888 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2999\r\n",
      "    Throughput: 999 infer/sec\r\n",
      "    Avg latency: 5998 usec (standard deviation 888 usec)\r\n",
      "    p50 latency: 6031 usec\r\n",
      "    p90 latency: 7179 usec\r\n",
      "    p95 latency: 7395 usec\r\n",
      "    p99 latency: 7629 usec\r\n",
      "    Avg gRPC time: 5949 usec (marshal 3 usec + response wait 5940 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3603\r\n",
      "    Avg request latency: 5616 usec (overhead 11 usec + queue 1674 usec + compute 3931 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 996 infer/sec. Avg latency: 7017 usec (std 717 usec)\r\n",
      "  Pass [2] throughput: 996 infer/sec. Avg latency: 7020 usec (std 673 usec)\r\n",
      "  Pass [3] throughput: 994 infer/sec. Avg latency: 7036 usec (std 775 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2983\r\n",
      "    Throughput: 994 infer/sec\r\n",
      "    Avg latency: 7036 usec (standard deviation 775 usec)\r\n",
      "    p50 latency: 7230 usec\r\n",
      "    p90 latency: 7868 usec\r\n",
      "    p95 latency: 7998 usec\r\n",
      "    p99 latency: 8403 usec\r\n",
      "    Avg gRPC time: 6993 usec (marshal 3 usec + response wait 6984 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3580\r\n",
      "    Avg request latency: 6650 usec (overhead 11 usec + queue 2686 usec + compute 3953 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 1000 infer/sec. Avg latency: 7989 usec (std 116 usec)\r\n",
      "  Pass [2] throughput: 995 infer/sec. Avg latency: 8030 usec (std 186 usec)\r\n",
      "  Pass [3] throughput: 999 infer/sec. Avg latency: 7994 usec (std 91 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2999\r\n",
      "    Throughput: 999 infer/sec\r\n",
      "    Avg latency: 7994 usec (standard deviation 91 usec)\r\n",
      "    p50 latency: 7991 usec\r\n",
      "    p90 latency: 8081 usec\r\n",
      "    p95 latency: 8116 usec\r\n",
      "    p99 latency: 8368 usec\r\n",
      "    Avg gRPC time: 7949 usec (marshal 3 usec + response wait 7941 usec + unmarshal 5 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3602\r\n",
      "    Avg request latency: 7623 usec (overhead 11 usec + queue 3682 usec + compute 3930 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 1005 infer/sec. Avg latency: 8939 usec (std 712 usec)\r\n",
      "  Pass [2] throughput: 998 infer/sec. Avg latency: 9007 usec (std 759 usec)\r\n",
      "  Pass [3] throughput: 1000 infer/sec. Avg latency: 8987 usec (std 797 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 3002\r\n",
      "    Throughput: 1000 infer/sec\r\n",
      "    Avg latency: 8987 usec (standard deviation 797 usec)\r\n",
      "    p50 latency: 8717 usec\r\n",
      "    p90 latency: 10117 usec\r\n",
      "    p95 latency: 10470 usec\r\n",
      "    p99 latency: 11508 usec\r\n",
      "    Avg gRPC time: 8945 usec (marshal 3 usec + response wait 8936 usec + unmarshal 6 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3602\r\n",
      "    Avg request latency: 8591 usec (overhead 11 usec + queue 4650 usec + compute 3930 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 1001 infer/sec. Avg latency: 9975 usec (std 1091 usec)\r\n",
      "  Pass [2] throughput: 1006 infer/sec. Avg latency: 9931 usec (std 884 usec)\r\n",
      "  Pass [3] throughput: 999 infer/sec. Avg latency: 9999 usec (std 902 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2998\r\n",
      "    Throughput: 999 infer/sec\r\n",
      "    Avg latency: 9999 usec (standard deviation 902 usec)\r\n",
      "    p50 latency: 10030 usec\r\n",
      "    p90 latency: 11180 usec\r\n",
      "    p95 latency: 11328 usec\r\n",
      "    p99 latency: 11739 usec\r\n",
      "    Avg gRPC time: 9959 usec (marshal 3 usec + response wait 9951 usec + unmarshal 5 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3599\r\n",
      "    Avg request latency: 9612 usec (overhead 10 usec + queue 5668 usec + compute 3934 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 232 infer/sec, latency 4307 usec\r\n",
      "Concurrency: 2, 460 infer/sec, latency 4334 usec\r\n",
      "Concurrency: 3, 688 infer/sec, latency 4356 usec\r\n",
      "Concurrency: 4, 904 infer/sec, latency 4416 usec\r\n",
      "Concurrency: 5, 1018 infer/sec, latency 4904 usec\r\n",
      "Concurrency: 6, 999 infer/sec, latency 5998 usec\r\n",
      "Concurrency: 7, 994 infer/sec, latency 7036 usec\r\n",
      "Concurrency: 8, 999 infer/sec, latency 7994 usec\r\n",
      "Concurrency: 9, 1000 infer/sec, latency 8987 usec\r\n",
      "Concurrency: 10, 999 infer/sec, latency 9999 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"large\" \"128\" \"fp16\" \"1\" \"1\" \"5000\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"large\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다. 이제 BERT 모델을 GPU를 이용하여 최적의 성능으로 Inference를 하실 수 있게 되셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Closing TRTIS container\n",
    "\n",
    "이제 아래 명령을 통해 실행한 container를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trt_server_cont\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f trt_server_cont"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
