{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# End-to-End BERT (Inference)\n",
    "\n",
    "이 문서는 TensorFlow BERT pretrained weight를 TensorRT engine으로 변환한 이후, TRTIS에 연동해서 Inference Serving하는 방법을 안내하기 위해 작성이 되었습니다. 이 문서에서 처리하는 절차는 크게 다음 두 가지 입니다.\n",
    "\n",
    "**1. BERT TensorRT inference engine build**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/deeplearning/tensorrt/trt-info.png\" width=\"600\" />\n",
    "TensorFlow이용하여 학습된 BERT pretrained weight를 TensorRT engine 파일로 변환합니다. 이 예제에서는 이 과정에서 필요한 plugin 들을 build하는 과정도 포함합니다. TensorRT를 이용한 BERT Sample에 대한 자세한 설명을 보시려면, [Real-Time Natural Language Understanding with BERT Using TensorRT](https://devblogs.nvidia.com/nlu-with-tensorrt-bert/)를 참고하세요.\n",
    "\n",
    "**2. TRTIS model repository 구성 및 TRTIS 서버 실행**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/pictures/2018/trt-inference-server-diagram-1200px.png\" width=\"600\" />\n",
    "이 예제에서는 완성된 engine 파일을 이용하여, TRTIS용 Model repository를 구성하고, TensorRT Inference Server를 실제로 구동하여 동작하는 것을 살펴볼 것입니다. TensorRT Inference Server에 대한 자세한 설명을 보시려면, [NVIDIA TensorRT Inference Server Boosts Deep Learning Inference](https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/) 문서를 참고하세요.\n",
    "\n",
    "이 문서는 DGX-1 V100 16GB 장비를 이용하여 테스트 되었으며, 구동 환경에 따라 성능은 다를 수 있습니다. 또한 사용된 SW는 다음과 같습니다.\n",
    "* CUDA 10.1 / CUDNN 7 / TensorRT 6.0\n",
    "* NGC Containers\n",
    "\n",
    "| 용도 | NGC container |\n",
    "|:---:|:---:|\n",
    "| TensorRT engine build | cuda:10.1-cudnn7-devel-ubuntu18.04 |\n",
    "| TensorRT Inference Server | tensorrtserver:12.10-py3 |\n",
    "| BERT inference client | tensorflow:12.08-py3 |\n",
    "    \n",
    "* docker / nvidia-docker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Building BERT TensorRT Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build TensorRT Docker Container\n",
    "\n",
    "TensorRT engine을 build하기 위해 우선 build 환경을 구축하기 위한 docker image를 생성합니다. 여기서는 TensorRT 6.0 BERT inference 예제를 이용할 것이며, 기준 환경인 CUDA 10.1 / TensorRT 6.0을 이용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  69.86MB\r",
      "\r\n",
      "Step 1/18 : ARG FROM_IMAGE_NAME\n",
      "Step 2/18 : FROM ${FROM_IMAGE_NAME}\n",
      " ---> b4879c167fc1\n",
      "Step 3/18 : ARG myuid\n",
      " ---> Using cache\n",
      " ---> b291f5ed61f5\n",
      "Step 4/18 : ARG mygid\n",
      " ---> Using cache\n",
      " ---> d7349efd9f7f\n",
      "Step 5/18 : RUN echo $myuid $mygid\n",
      " ---> Using cache\n",
      " ---> c123c6a80ff1\n",
      "Step 6/18 : RUN groupadd -r --gid ${mygid} bert && useradd -r -u ${myuid} --gid ${mygid} -ms /bin/bash bert\n",
      " ---> Using cache\n",
      " ---> 5641f0f47c13\n",
      "Step 7/18 : RUN echo 'bert:bert' | chpasswd\n",
      " ---> Using cache\n",
      " ---> c150681050ce\n",
      "Step 8/18 : RUN mkdir -p /workspace && chown -R bert /workspace\n",
      " ---> Using cache\n",
      " ---> a693573784bc\n",
      "Step 9/18 : ARG TRT_PKG_VERSION=7.0.0-1+cuda10.2\n",
      " ---> Using cache\n",
      " ---> 3f5d7f357952\n",
      "Step 10/18 : RUN apt-get update && apt-get install -y --no-install-recommends     software-properties-common     pbzip2 pv bzip2  sudo gcc-7 g++-7  zlib1g-dev      unzip     libcurl4-openssl-dev     wget     zlib1g-dev     git     pkg-config     python3     python3-dev     python3-pip     python3-setuptools     python3-wheel     libnvinfer6=${TRT_PKG_VERSION}     libnvinfer-dev=${TRT_PKG_VERSION}     libnvinfer-plugin6=${TRT_PKG_VERSION}     libnvinfer-plugin-dev=${TRT_PKG_VERSION}     libnvparsers6=${TRT_PKG_VERSION}     libnvparsers-dev=${TRT_PKG_VERSION}     libnvonnxparsers6=${TRT_PKG_VERSION}     libnvonnxparsers-dev=${TRT_PKG_VERSION}     python3-libnvinfer=${TRT_PKG_VERSION}     python3-libnvinfer-dev=${TRT_PKG_VERSION}\n",
      " ---> Using cache\n",
      " ---> d4225488d606\n",
      "Step 11/18 : RUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60                                  --slave /usr/bin/g++ g++ /usr/bin/g++-7  &&                                  update-alternatives --config gcc\n",
      " ---> Using cache\n",
      " ---> 6601f17fa97e\n",
      "Step 12/18 : RUN cd /tmp &&   wget https://github.com/Kitware/CMake/releases/download/v3.14.4/cmake-3.14.4-Linux-x86_64.sh &&   chmod +x cmake-3.14.4-Linux-x86_64.sh &&   ./cmake-3.14.4-Linux-x86_64.sh --prefix=/usr/local --exclude-subdir --skip-license &&   rm ./cmake-3.14.4-Linux-x86_64.sh\n",
      " ---> Using cache\n",
      " ---> 013c49a0b7d1\n",
      "Step 13/18 : RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1 &&     update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 1ec89a3443ff\n",
      "Step 14/18 : RUN pip install tensorflow==1.14.0 &&     pip install pycuda\n",
      " ---> Using cache\n",
      " ---> 626bf9152a12\n",
      "Step 15/18 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 54ce231c8209\n",
      "Step 16/18 : USER bert\n",
      " ---> Using cache\n",
      " ---> 8ad70a625cb8\n",
      "Step 17/18 : WORKDIR /workspace\n",
      " ---> Using cache\n",
      " ---> 8cee2eb13a8f\n",
      "Step 18/18 : CMD /bin/bash\n",
      " ---> Using cache\n",
      " ---> d58c4480143e\n",
      "Successfully built d58c4480143e\n",
      "Successfully tagged bert_trt:latest\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../trt\n",
    "docker build . -f Dockerfile -t bert_trt --rm \\\n",
    "    --build-arg FROM_IMAGE_NAME=nvcr.io/nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 \\\n",
    "    --build-arg TRT_PKG_VERSION=6.0.1-1+cuda10.1 \\\n",
    "    --build-arg myuid=$(id -u) --build-arg mygid=1000 # $(id -g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Container 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 docker 실행 명령을 이용하여 BERT TensorRT engine을 build하기 위한 container를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4112a8277e0476775e7a2b6a5c22d84475e91e491ea80b25fb3f0f11946cf567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: No such container: bert_trt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "\n",
    "GPU_ID=${1:-\"ALL\"}\n",
    "\n",
    "ENGINE_OUTPUT_DIR=\"outputs\"\n",
    "\n",
    "if [[ ! -e ${ENGINE_OUTPUT_DIR} ]]; then\n",
    "    mkdir -p ${ENGINE_OUTPUT_DIR}\n",
    "fi\n",
    "\n",
    "docker rm -f bert_trt\n",
    "docker run -d -ti \\\n",
    "    --name bert_trt${VERSION} \\\n",
    "    --runtime=nvidia \\\n",
    "    --shm-size=1g --ulimit memlock=1 --ulimit stack=67108864 \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=${GPU_ID} \\\n",
    "    -v $(pwd)/outputs:/workspace/outputs \\\n",
    "    -v $(pwd)/results/models:/workspace/models \\\n",
    "    -v $(pwd)/trt/TensorRT:/workspace/TensorRT \\\n",
    "    bert_trt bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                  PORTS               NAMES\r\n",
      "4112a8277e04        bert_trt            \"bash\"              1 second ago        Up Less than a second                       bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build TensorRT Plugin Layer library and download pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT TensorRT engine을 build하기 위해 필요한 plugin의 라이브러리와 예제를 위해 pretrained-weight를 ngc로부터 다운로드 받습니다.\n",
    "\n",
    "#### 1. Plugin build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building TensorRT plugins for BERT\r\n",
      "-- The CXX compiler identification is GNU 7.4.0\r\n",
      "-- The CUDA compiler identification is NVIDIA 10.1.243\r\n",
      "-- Check for working CXX compiler: /usr/bin/c++\r\n",
      "-- Check for working CXX compiler: /usr/bin/c++ -- works\r\n",
      "-- Detecting CXX compiler ABI info\r\n",
      "-- Detecting CXX compiler ABI info - done\r\n",
      "-- Detecting CXX compile features\r\n",
      "-- Detecting CXX compile features - done\r\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc\r\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc -- works\r\n",
      "-- Detecting CUDA compiler ABI info\r\n",
      "-- Detecting CUDA compiler ABI info - done\r\n",
      "-- Configuring done\r\n",
      "-- Generating done\r\n",
      "-- Build files have been written to: /workspace/TensorRT/demo/BERT/build\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target common\u001b[0m\r\n",
      "[  7%] \u001b[32mBuilding CXX object CMakeFiles/common.dir/workspace/TensorRT/samples/common/logger.cpp.o\u001b[0m\r\n",
      "[ 15%] \u001b[32m\u001b[1mLinking CXX shared library libcommon.so\u001b[0m\r\n",
      "[ 15%] Built target common\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target bert_plugins\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/geluPlugin.cu.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/embLayerNormPlugin.cu.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/qkvToContextPlugin.cu.o\u001b[0m\r\n",
      "[ 46%] \u001b[32mBuilding CUDA object CMakeFiles/bert_plugins.dir/plugins/skipLayerNormPlugin.cu.o\u001b[0m\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const __nv_bool *, const __nv_bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(47): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::GeluPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(51): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::GeluPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(53): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::GeluPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/geluPlugin.h(55): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::GeluPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(48): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/embLayerNormPlugin.h(56): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::EmbLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(49): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(53): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::QKVToContextPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(55): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::QKVToContextPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/qkvToContextPlugin.h(57): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::QKVToContextPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(46): warning: function \"nvinfer1::IPluginV2::getOutputDimensions(int, const nvinfer1::Dims *, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getOutputDimensions\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(50): warning: function \"nvinfer1::IPluginV2Ext::configurePlugin(const nvinfer1::Dims *, int, const nvinfer1::Dims *, int, const nvinfer1::DataType *, const nvinfer1::DataType *, const bool *, const bool *, nvinfer1::PluginFormat, int)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::configurePlugin\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(52): warning: function \"nvinfer1::IPluginV2::getWorkspaceSize(int) const\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::getWorkspaceSize\" -- virtual function override intended?\r\n",
      "\r\n",
      "/workspace/TensorRT/demo/BERT/plugins/skipLayerNormPlugin.h(54): warning: function \"nvinfer1::IPluginV2::enqueue(int, const void *const *, void **, void *, cudaStream_t)\" is hidden by \"bert::test::SkipLayerNormPluginDynamic::enqueue\" -- virtual function override intended?\r\n",
      "\r\n",
      "[ 53%] \u001b[32m\u001b[1mLinking CUDA device code CMakeFiles/bert_plugins.dir/cmake_device_link.o\u001b[0m\r\n",
      "[ 61%] \u001b[32m\u001b[1mLinking CUDA shared library libbert_plugins.so\u001b[0m\r\n",
      "[ 61%] Built target bert_plugins\r\n",
      "\u001b[35m\u001b[1mScanning dependencies of target sample_bert\u001b[0m\r\n",
      "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/bert/bert.cpp.o\u001b[0m\r\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/bert/driver.cpp.o\u001b[0m\r\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/util/dataUtils.cpp.o\u001b[0m\r\n",
      "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/sample_bert.dir/sampleBERT.cpp.o\u001b[0m\r\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable sample_bert\u001b[0m\r\n",
      "[100%] Built target sample_bert\r\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [[ -e ../trt/TensorRT/demo/BERT/build ]]; then\n",
    "    rm -rf ../trt/TensorRT/demo/BERT/build\n",
    "fi\n",
    "docker exec -t bert_trt bash /workspace/TensorRT/demo/BERT/python/build_plugins.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실행결과 아래 경로에 build 한 결과가 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMakeCache.txt\tcmake_install.cmake  libcommon.so  sample_bert\r\n",
      "CMakeFiles\tlibbert_plugins.so   Makefile\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../trt/TensorRT/demo/BERT/build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-trained weight download\n",
    "NGC로부터 pre-trained weight를 다운로드 받습니다. NGC에서는 다음의 조건에 대한 pretrained weight를 제공하므로 이 중에 선택하여 사용할 수 있습니다.\n",
    "\n",
    "| | options |\n",
    "|:---:|:---:|\n",
    "| model | large, base |\n",
    "| precision | fp32, fp16 |\n",
    "| seq. length | 128, 384 |\n",
    "\n",
    "물론 독자적으로 학습하신 weight (ckpt)를 사용하실 수도 있습니다.\n",
    "\n",
    "이 예제에서는 bert-large, fp16, seq-len 128 을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'base' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "if [[ ! -d ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/ ]]; then\n",
    "    docker exec -t bert_trt bash /workspace/TensorRT/demo/BERT/python/download_fine-tuned_model.sh ${MODEL} ${FT_PRECISION} ${SEQ_LEN}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGC에서 다운로드 받은 pretrained weight는 ```/results/models/fine-tuned/```에 저장이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json\n",
      "model.ckpt-8144.data-00000-of-00001\n",
      "model.ckpt-8144.index\n",
      "model.ckpt-8144.meta\n",
      "tf_bert_squad_1n_fp16_gbs32.190523114449.log\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'base' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "ls ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 BERT TensorRT precision을 build할 것입니다. 이전에 NGC model repository에서 다운로드 받은 pretrained weight와 동일한 조건으로 engine을 build하되 batch size를 지정해 줘야 합니다. 현재 버전에서는 Batch size 1만을 지원하므로, 여기서는 batch size를 1로만 지정해서 테스트 하도록 하겠습니다. 실행시간이 Telsa V100 기준으로 **20분** 가량 소요되므로 신중히 실행하시기 바랍니다.\n",
    "\n",
    "참고로 별도의 terminal 창을 열어서 ```watch -n1 nvidia-smi```를 이용해서 TensorRT가 engine을 Build하면서 GPU를 점유하는 것을 보실 수 있습니다. 이 과정에서 GPU를 사용하는 이유는 engine을 build하는 과정에서 target GPU의 성능을 측정하여 적절한 GPU Kernel의 구성을 TensorRT가 찾기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: Using configuration file: /workspace/models/fine-tuned/bert_tf_v2_base_fp16_128_v2/bert_config.json\n",
      "[TensorRT] INFO: Found 202 entries in weight map\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Saving Engine to /workspace/outputs/bert_base_128.engine\n",
      "[TensorRT] INFO: Done.\n",
      "CPU times: user 12.5 ms, sys: 12.3 ms, total: 24.8 ms\n",
      "Wall time: 7min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash -s 'base' 'fp16' '128' '1' \n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_builder.py \\\n",
    "            -m /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/model.ckpt-8144 \\\n",
    "            -c /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2 \\\n",
    "            -o /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine \\\n",
    "            -s ${SEQ_LEN} -b ${BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 Script의 실행결과 아래 경로에 engine 파일이 생성된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_128.engine  bert_large_128.engine\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti bert_trt ls /workspace/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 build 한 TensorRT engine을 이용해서 inference를 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\r\n",
      "WARNING:tensorflow:From /workspace/TensorRT/demo/BERT/python/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "\r\n",
      "Question: What is TensorRT?\r\n",
      "\r\n",
      "Running Inference...\r\n",
      "------------------------\r\n",
      "Running inference in 215.236 Sentences/Sec\r\n",
      "------------------------\r\n",
      "Processing output 0 in batch\r\n",
      "Answer: 'high performance deep learning inference platform'\r\n",
      "With probability: 29.156\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'base' 'fp16' '128' '1'\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_inference.py \\\n",
    "            -e /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine -s ${SEQ_LEN} \\\n",
    "            -p \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\" \\\n",
    "            -q \"What is TensorRT?\" \\\n",
    "            -v /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Closing the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f bert_trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT inferencing platform with TRTIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pulling TRTIS docker image\n",
    "\n",
    "TensorRT Inference Server는 새로운 Build 없이 Serving을 할 수 있는 장점이 있습니다. 우선 원활한 예제의 실행을 위해 사용할 이미지를 다음 명령을 이용하여 pull 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.10-py3: Pulling from nvidia/tensorrtserver\n",
      "5667fdb72017: Pulling fs layer\n",
      "d83811f270d5: Pulling fs layer\n",
      "ee671aafb583: Pulling fs layer\n",
      "7fc152dfb3a6: Pulling fs layer\n",
      "dbc57626691b: Pulling fs layer\n",
      "e20092842144: Pulling fs layer\n",
      "d64c76da70d5: Pulling fs layer\n",
      "429f0b34bf97: Pulling fs layer\n",
      "39d853a0098c: Pulling fs layer\n",
      "dc9dfc23df66: Pulling fs layer\n",
      "1a32524cb863: Pulling fs layer\n",
      "dbc57626691b: Waiting\n",
      "e20092842144: Waiting\n",
      "7fc152dfb3a6: Waiting\n",
      "d3d394313ced: Pulling fs layer\n",
      "857b6050fd78: Pulling fs layer\n",
      "dc9dfc23df66: Waiting\n",
      "1a32524cb863: Waiting\n",
      "3a51649b9b50: Pulling fs layer\n",
      "885e286ed6cc: Pulling fs layer\n",
      "3a51649b9b50: Waiting\n",
      "885e286ed6cc: Waiting\n",
      "62be33d17790: Pulling fs layer\n",
      "6a7d05a28b83: Pulling fs layer\n",
      "11ff4c1b1e9b: Pulling fs layer\n",
      "252fb308c785: Pulling fs layer\n",
      "6a7d05a28b83: Waiting\n",
      "11ff4c1b1e9b: Waiting\n",
      "4749ee710260: Pulling fs layer\n",
      "47668c0cb079: Pulling fs layer\n",
      "4749ee710260: Waiting\n",
      "4f9ec6b1521d: Pulling fs layer\n",
      "292b425b68e8: Pulling fs layer\n",
      "47668c0cb079: Waiting\n",
      "93e46b746825: Pulling fs layer\n",
      "4f9ec6b1521d: Waiting\n",
      "d66e2a94ffdd: Pulling fs layer\n",
      "292b425b68e8: Waiting\n",
      "93e46b746825: Waiting\n",
      "9ec0ad11e3f4: Pulling fs layer\n",
      "28efceee1d39: Pulling fs layer\n",
      "9ec0ad11e3f4: Waiting\n",
      "026a283c83f0: Pulling fs layer\n",
      "d66e2a94ffdd: Waiting\n",
      "28efceee1d39: Waiting\n",
      "af0f2fe8c66a: Pulling fs layer\n",
      "ef30f655718e: Pulling fs layer\n",
      "0b20230b4afa: Pulling fs layer\n",
      "026a283c83f0: Waiting\n",
      "bd575020981a: Pulling fs layer\n",
      "27fab5730dad: Pulling fs layer\n",
      "0b20230b4afa: Waiting\n",
      "bd575020981a: Waiting\n",
      "d6e887bc890d: Pulling fs layer\n",
      "aad3f5aa61ff: Pulling fs layer\n",
      "d6e887bc890d: Waiting\n",
      "865e767514bd: Pulling fs layer\n",
      "aad3f5aa61ff: Waiting\n",
      "d02230245f58: Pulling fs layer\n",
      "8e2c5df66c6c: Pulling fs layer\n",
      "47f4926fba6a: Pulling fs layer\n",
      "615aae853aba: Pulling fs layer\n",
      "47f4926fba6a: Waiting\n",
      "de3ab30e88da: Pulling fs layer\n",
      "615aae853aba: Waiting\n",
      "8e2c5df66c6c: Waiting\n",
      "e5fd197ed72d: Pulling fs layer\n",
      "ff3f6661e077: Pulling fs layer\n",
      "de3ab30e88da: Waiting\n",
      "e5fd197ed72d: Waiting\n",
      "ff3f6661e077: Waiting\n",
      "d83811f270d5: Verifying Checksum\n",
      "d83811f270d5: Download complete\n",
      "ee671aafb583: Verifying Checksum\n",
      "ee671aafb583: Download complete\n",
      "7fc152dfb3a6: Download complete\n",
      "e20092842144: Download complete\n",
      "dbc57626691b: Verifying Checksum\n",
      "dbc57626691b: Download complete\n",
      "429f0b34bf97: Download complete\n",
      "5667fdb72017: Verifying Checksum\n",
      "5667fdb72017: Download complete\n",
      "39d853a0098c: Verifying Checksum\n",
      "39d853a0098c: Download complete\n",
      "dc9dfc23df66: Verifying Checksum\n",
      "dc9dfc23df66: Download complete\n",
      "1a32524cb863: Verifying Checksum\n",
      "1a32524cb863: Download complete\n",
      "857b6050fd78: Verifying Checksum\n",
      "857b6050fd78: Download complete\n",
      "3a51649b9b50: Verifying Checksum\n",
      "3a51649b9b50: Download complete\n",
      "885e286ed6cc: Verifying Checksum\n",
      "885e286ed6cc: Download complete\n",
      "62be33d17790: Verifying Checksum\n",
      "62be33d17790: Download complete\n",
      "6a7d05a28b83: Verifying Checksum\n",
      "6a7d05a28b83: Download complete\n",
      "5667fdb72017: Pull complete\n",
      "d83811f270d5: Pull complete\n",
      "ee671aafb583: Pull complete\n",
      "7fc152dfb3a6: Pull complete\n",
      "dbc57626691b: Pull complete\n",
      "e20092842144: Pull complete\n",
      "11ff4c1b1e9b: Verifying Checksum\n",
      "11ff4c1b1e9b: Download complete\n",
      "252fb308c785: Verifying Checksum\n",
      "252fb308c785: Download complete\n",
      "4749ee710260: Verifying Checksum\n",
      "4749ee710260: Download complete\n",
      "47668c0cb079: Verifying Checksum\n",
      "47668c0cb079: Download complete\n",
      "4f9ec6b1521d: Download complete\n",
      "292b425b68e8: Verifying Checksum\n",
      "292b425b68e8: Download complete\n",
      "d3d394313ced: Verifying Checksum\n",
      "d3d394313ced: Download complete\n",
      "d66e2a94ffdd: Verifying Checksum\n",
      "d66e2a94ffdd: Download complete\n",
      "d64c76da70d5: Verifying Checksum\n",
      "d64c76da70d5: Download complete\n",
      "28efceee1d39: Verifying Checksum\n",
      "28efceee1d39: Download complete\n",
      "026a283c83f0: Download complete\n",
      "af0f2fe8c66a: Download complete\n",
      "ef30f655718e: Verifying Checksum\n",
      "ef30f655718e: Download complete\n",
      "9ec0ad11e3f4: Verifying Checksum\n",
      "9ec0ad11e3f4: Download complete\n",
      "0b20230b4afa: Download complete\n",
      "27fab5730dad: Download complete\n",
      "bd575020981a: Download complete\n",
      "aad3f5aa61ff: Verifying Checksum\n",
      "aad3f5aa61ff: Download complete\n",
      "d64c76da70d5: Pull complete\n",
      "93e46b746825: Verifying Checksum\n",
      "93e46b746825: Download complete\n",
      "429f0b34bf97: Pull complete\n",
      "39d853a0098c: Pull complete\n",
      "d02230245f58: Download complete\n",
      "dc9dfc23df66: Pull complete\n",
      "8e2c5df66c6c: Download complete\n",
      "1a32524cb863: Pull complete\n",
      "47f4926fba6a: Verifying Checksum\n",
      "47f4926fba6a: Download complete\n",
      "615aae853aba: Verifying Checksum\n",
      "615aae853aba: Download complete\n",
      "de3ab30e88da: Verifying Checksum\n",
      "de3ab30e88da: Download complete\n",
      "e5fd197ed72d: Download complete\n",
      "ff3f6661e077: Verifying Checksum\n",
      "ff3f6661e077: Download complete\n",
      "d3d394313ced: Pull complete\n",
      "857b6050fd78: Pull complete\n",
      "3a51649b9b50: Pull complete\n",
      "885e286ed6cc: Pull complete\n",
      "62be33d17790: Pull complete\n",
      "6a7d05a28b83: Pull complete\n",
      "11ff4c1b1e9b: Pull complete\n",
      "252fb308c785: Pull complete\n",
      "4749ee710260: Pull complete\n",
      "47668c0cb079: Pull complete\n",
      "4f9ec6b1521d: Pull complete\n",
      "292b425b68e8: Pull complete\n",
      "d6e887bc890d: Verifying Checksum\n",
      "d6e887bc890d: Download complete\n",
      "865e767514bd: Verifying Checksum\n",
      "865e767514bd: Download complete\n",
      "93e46b746825: Pull complete\n",
      "d66e2a94ffdd: Pull complete\n",
      "9ec0ad11e3f4: Pull complete\n",
      "28efceee1d39: Pull complete\n",
      "026a283c83f0: Pull complete\n",
      "af0f2fe8c66a: Pull complete\n",
      "ef30f655718e: Pull complete\n",
      "0b20230b4afa: Pull complete\n",
      "bd575020981a: Pull complete\n",
      "27fab5730dad: Pull complete\n",
      "d6e887bc890d: Pull complete\n",
      "aad3f5aa61ff: Pull complete\n",
      "865e767514bd: Pull complete\n",
      "d02230245f58: Pull complete\n",
      "8e2c5df66c6c: Pull complete\n",
      "47f4926fba6a: Pull complete\n",
      "615aae853aba: Pull complete\n",
      "de3ab30e88da: Pull complete\n",
      "e5fd197ed72d: Pull complete\n",
      "ff3f6661e077: Pull complete\n",
      "Digest: sha256:3d44d3a2e27b6c30ebbd693ddd38753e6c4645d9706968612296233f76be308f\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tensorrtserver:19.10-py3\n",
      "nvcr.io/nvidia/tensorrtserver:19.10-py3\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "docker pull nvcr.io/nvidia/tensorrtserver:19.10-py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting TRTIS model repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 명령들을 이용하여 TRTIS model repository를 구성합니다. 여기서 숫자 1은 model version으로 원하는 버전을 설정하실 수 있으며, 향후에 inference client 단에서 원하는 버전을 지정하여 inference가 되도록 지정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../results/trtis_models/bert_base_128_fp16\n",
    "mkdir -p ../results/trtis_models/bert_base_128_fp16/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model repository에는 model를 명시하는 ```config.pbtxt``` 파일과 TensorRT engine 파일을 ```model.plan```으로 이름을 변경하여 버전에 따라 저장을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_base_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0]\n",
    "        profile: \"0\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp ../outputs/bert_base_128.engine ../results/trtis_models/bert_base_128_fp16/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model repository 구성과 함께 중요한 절차는 TensorRT를 이용하여 inference하는데 필요한 Plugin을 TRTIS에게 알려주는 것입니다. 그 이유는, TRTIS 입장에서는 TensorRT engine에서 사용하는 plugin의 정보를 사전에 알 방법이 없기 때문입니다. 이런 library는 여러개로 늘어날 수 있으므로 BERT 예제의 build directory에 두지 않고 공용으로 관리하기 좋은 별도의 공간으로 옮기도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../trt/plugins\n",
    "cp ../trt/TensorRT/demo/BERT/build/*.so ../trt/plugins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launch Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TensorRT inference server를 구동시킬 차례입니다. TensorFlow Model이 FP16으로 Inference 되도록 하게 하는 한편, TensorRT engine의 dependency를 위한 경로를 설정해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp16 activated!\n",
      "c7dc3453ee805fb2c4af6b0aade106b0cd1ddfc50dafb31a9311b5c040e1af7d\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"fp16\"\n",
    "\n",
    "cd ..\n",
    "\n",
    "precision=${1:-\"fp16\"}\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "   echo \"fp16 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1\n",
    "else\n",
    "   echo \"fp32 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=0\n",
    "fi\n",
    "\n",
    "# Start TRTIS server in detached state\n",
    "docker run -d --rm \\\n",
    "   --runtime=nvidia \\\n",
    "   --shm-size=1g \\\n",
    "   --ulimit memlock=-1 \\\n",
    "   --ulimit stack=67108864 \\\n",
    "   -p8000:8000 \\\n",
    "   -p8001:8001 \\\n",
    "   -p8002:8002 \\\n",
    "   --name trt_server_cont \\\n",
    "   -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "   -e TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE \\\n",
    "   -v $(pwd)/results/trtis_models:/models \\\n",
    "   -v $(pwd)/trt/plugins:/opt/tensorrtserver/lib/plugins \\\n",
    "   -e LD_PRELOAD=\"/opt/tensorrtserver/lib/plugins/libcommon.so:/opt/tensorrtserver/lib/plugins/libbert_plugins.so\" \\\n",
    "   nvcr.io/nvidia/tensorrtserver:19.10-py3 \\\n",
    "        trtserver --model-store=/models --strict-model-config=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BERT client build (optional)\n",
    "만약 BERT Training을 위해 사용한 BERT docker image가 위치한 노드와 동일한 노드라면 다음 명령은 생략하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.08-py3: Pulling from nvidia/tensorrtserver\n",
      "7413c47ba209: Pulling fs layer\n",
      "0fe7e7cbb2e8: Pulling fs layer\n",
      "1d425c982345: Pulling fs layer\n",
      "344da5c95cec: Pulling fs layer\n",
      "ae62549b429d: Pulling fs layer\n",
      "e275e0ef6c20: Pulling fs layer\n",
      "4090c4d315fe: Pulling fs layer\n",
      "00a11b299176: Pulling fs layer\n",
      "74a29ca83919: Pulling fs layer\n",
      "a1abd2d74110: Pulling fs layer\n",
      "90d7249fe09b: Pulling fs layer\n",
      "5db1b1a35ea4: Pulling fs layer\n",
      "344da5c95cec: Waiting\n",
      "b160969adc93: Pulling fs layer\n",
      "ae62549b429d: Waiting\n",
      "0179f14b1047: Pulling fs layer\n",
      "e275e0ef6c20: Waiting\n",
      "a58b5dcd3fa6: Pulling fs layer\n",
      "4090c4d315fe: Waiting\n",
      "e7af950e37dd: Pulling fs layer\n",
      "74a29ca83919: Waiting\n",
      "90d7249fe09b: Waiting\n",
      "e880be2d991d: Pulling fs layer\n",
      "a1abd2d74110: Waiting\n",
      "00a11b299176: Waiting\n",
      "b160969adc93: Waiting\n",
      "0179f14b1047: Waiting\n",
      "a58b5dcd3fa6: Waiting\n",
      "b7c0ae26dc75: Pulling fs layer\n",
      "5db1b1a35ea4: Waiting\n",
      "e7af950e37dd: Waiting\n",
      "423736729fa4: Pulling fs layer\n",
      "9595d4b4fa6d: Pulling fs layer\n",
      "d18ab9b3cee4: Pulling fs layer\n",
      "d13f74634ff4: Pulling fs layer\n",
      "6465f099eaee: Pulling fs layer\n",
      "b7c0ae26dc75: Waiting\n",
      "9595d4b4fa6d: Waiting\n",
      "423736729fa4: Waiting\n",
      "1d25a5143caf: Pulling fs layer\n",
      "1488e34e1ef6: Pulling fs layer\n",
      "6465f099eaee: Waiting\n",
      "d18ab9b3cee4: Waiting\n",
      "1d25a5143caf: Waiting\n",
      "c0b9035f7b0d: Pulling fs layer\n",
      "e12a027580b2: Pulling fs layer\n",
      "c0b9035f7b0d: Waiting\n",
      "2195a5a8e51b: Pulling fs layer\n",
      "68d9a4bdc44b: Pulling fs layer\n",
      "79ac09aadede: Pulling fs layer\n",
      "4dfca455860d: Pulling fs layer\n",
      "2195a5a8e51b: Waiting\n",
      "68d9a4bdc44b: Waiting\n",
      "8031f1622bfe: Pulling fs layer\n",
      "d70a9aeed337: Pulling fs layer\n",
      "60cf0e9e8c63: Pulling fs layer\n",
      "8aafc5cadacf: Pulling fs layer\n",
      "60cf0e9e8c63: Waiting\n",
      "6d9af9715b5a: Pulling fs layer\n",
      "a55309038303: Pulling fs layer\n",
      "8aafc5cadacf: Waiting\n",
      "a55309038303: Waiting\n",
      "1d425c982345: Verifying Checksum\n",
      "1d425c982345: Download complete\n",
      "0fe7e7cbb2e8: Verifying Checksum\n",
      "0fe7e7cbb2e8: Download complete\n",
      "344da5c95cec: Verifying Checksum\n",
      "344da5c95cec: Download complete\n",
      "e275e0ef6c20: Download complete\n",
      "7413c47ba209: Verifying Checksum\n",
      "7413c47ba209: Download complete\n",
      "ae62549b429d: Verifying Checksum\n",
      "ae62549b429d: Download complete\n",
      "00a11b299176: Verifying Checksum\n",
      "00a11b299176: Download complete\n",
      "74a29ca83919: Download complete\n",
      "90d7249fe09b: Verifying Checksum\n",
      "90d7249fe09b: Download complete\n",
      "a1abd2d74110: Download complete\n",
      "b160969adc93: Verifying Checksum\n",
      "b160969adc93: Download complete\n",
      "0179f14b1047: Download complete\n",
      "a58b5dcd3fa6: Verifying Checksum\n",
      "a58b5dcd3fa6: Download complete\n",
      "7413c47ba209: Pull complete\n",
      "0fe7e7cbb2e8: Pull complete\n",
      "e7af950e37dd: Verifying Checksum\n",
      "e7af950e37dd: Download complete\n",
      "1d425c982345: Pull complete\n",
      "e880be2d991d: Download complete\n",
      "344da5c95cec: Pull complete\n",
      "ae62549b429d: Pull complete\n",
      "e275e0ef6c20: Pull complete\n",
      "b7c0ae26dc75: Verifying Checksum\n",
      "b7c0ae26dc75: Download complete\n",
      "423736729fa4: Verifying Checksum\n",
      "423736729fa4: Download complete\n",
      "9595d4b4fa6d: Download complete\n",
      "d18ab9b3cee4: Verifying Checksum\n",
      "d18ab9b3cee4: Download complete\n",
      "d13f74634ff4: Download complete\n",
      "6465f099eaee: Verifying Checksum\n",
      "6465f099eaee: Download complete\n",
      "5db1b1a35ea4: Verifying Checksum\n",
      "5db1b1a35ea4: Download complete\n",
      "1488e34e1ef6: Download complete\n",
      "c0b9035f7b0d: Verifying Checksum\n",
      "c0b9035f7b0d: Download complete\n",
      "e12a027580b2: Verifying Checksum\n",
      "e12a027580b2: Download complete\n",
      "2195a5a8e51b: Verifying Checksum\n",
      "2195a5a8e51b: Download complete\n",
      "68d9a4bdc44b: Download complete\n",
      "79ac09aadede: Download complete\n",
      "4dfca455860d: Verifying Checksum\n",
      "4dfca455860d: Download complete\n",
      "8031f1622bfe: Verifying Checksum\n",
      "8031f1622bfe: Download complete\n",
      "1d25a5143caf: Verifying Checksum\n",
      "1d25a5143caf: Download complete\n",
      "60cf0e9e8c63: Download complete\n",
      "4090c4d315fe: Verifying Checksum\n",
      "4090c4d315fe: Download complete\n",
      "6d9af9715b5a: Download complete\n",
      "a55309038303: Verifying Checksum\n",
      "a55309038303: Download complete\n",
      "8aafc5cadacf: Verifying Checksum\n",
      "8aafc5cadacf: Download complete\n",
      "4090c4d315fe: Pull complete\n",
      "00a11b299176: Pull complete\n",
      "74a29ca83919: Pull complete\n",
      "a1abd2d74110: Pull complete\n",
      "90d7249fe09b: Pull complete\n",
      "d70a9aeed337: Verifying Checksum\n",
      "d70a9aeed337: Download complete\n",
      "5db1b1a35ea4: Pull complete\n",
      "b160969adc93: Pull complete\n",
      "0179f14b1047: Pull complete\n",
      "a58b5dcd3fa6: Pull complete\n",
      "e7af950e37dd: Pull complete\n",
      "e880be2d991d: Pull complete\n",
      "b7c0ae26dc75: Pull complete\n",
      "423736729fa4: Pull complete\n",
      "9595d4b4fa6d: Pull complete\n",
      "d18ab9b3cee4: Pull complete\n",
      "d13f74634ff4: Pull complete\n",
      "6465f099eaee: Pull complete\n",
      "1d25a5143caf: Pull complete\n",
      "1488e34e1ef6: Pull complete\n",
      "c0b9035f7b0d: Pull complete\n",
      "e12a027580b2: Pull complete\n",
      "2195a5a8e51b: Pull complete\n",
      "68d9a4bdc44b: Pull complete\n",
      "79ac09aadede: Pull complete\n",
      "4dfca455860d: Pull complete\n",
      "8031f1622bfe: Pull complete\n",
      "d70a9aeed337: Pull complete\n",
      "60cf0e9e8c63: Pull complete\n",
      "8aafc5cadacf: Pull complete\n",
      "6d9af9715b5a: Pull complete\n",
      "a55309038303: Pull complete\n",
      "Digest: sha256:438b6c2ddfd095faf3453f348c8639ea5be0c28a687a604d6f691f07469076c6\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tensorrtserver:19.08-py3\n",
      "nvcr.io/nvidia/tensorrtserver:19.08-py3\n",
      "Sending build context to Docker daemon  2.762GB\r",
      "\r\n",
      "Step 1/19 : ARG FROM_IMAGE_NAME=nvcr.io/nvidia/tensorflow:19.08-py3\n",
      "Step 2/19 : FROM ${FROM_IMAGE_NAME}\n",
      "19.08-py3: Pulling from nvidia/tensorflow\n",
      "7413c47ba209: Already exists\n",
      "0fe7e7cbb2e8: Already exists\n",
      "1d425c982345: Already exists\n",
      "344da5c95cec: Already exists\n",
      "ae62549b429d: Already exists\n",
      "e275e0ef6c20: Already exists\n",
      "4090c4d315fe: Already exists\n",
      "00a11b299176: Already exists\n",
      "74a29ca83919: Already exists\n",
      "a1abd2d74110: Already exists\n",
      "90d7249fe09b: Already exists\n",
      "5db1b1a35ea4: Already exists\n",
      "b160969adc93: Already exists\n",
      "0179f14b1047: Already exists\n",
      "a58b5dcd3fa6: Already exists\n",
      "e7af950e37dd: Already exists\n",
      "e880be2d991d: Already exists\n",
      "b7c0ae26dc75: Already exists\n",
      "423736729fa4: Already exists\n",
      "9595d4b4fa6d: Already exists\n",
      "d18ab9b3cee4: Already exists\n",
      "d13f74634ff4: Already exists\n",
      "6465f099eaee: Already exists\n",
      "1d25a5143caf: Already exists\n",
      "b863ed055d32: Pulling fs layer\n",
      "d7c7f6dae015: Pulling fs layer\n",
      "d16fb958ffce: Pulling fs layer\n",
      "dc1d1b858f8d: Pulling fs layer\n",
      "6ba9ceffc16c: Pulling fs layer\n",
      "9736392613d1: Pulling fs layer\n",
      "379930b0d052: Pulling fs layer\n",
      "74848a82745a: Pulling fs layer\n",
      "45feee628410: Pulling fs layer\n",
      "89a8956c6c35: Pulling fs layer\n",
      "dc1d1b858f8d: Waiting\n",
      "abce4c168c8f: Pulling fs layer\n",
      "c05bf1206572: Pulling fs layer\n",
      "9736392613d1: Waiting\n",
      "6ba9ceffc16c: Waiting\n",
      "c275ba3d2b9d: Pulling fs layer\n",
      "74848a82745a: Waiting\n",
      "379930b0d052: Waiting\n",
      "0dcd07dd2212: Pulling fs layer\n",
      "81eea7d16450: Pulling fs layer\n",
      "4ee086c2d8f9: Pulling fs layer\n",
      "16c72bfe563b: Pulling fs layer\n",
      "c275ba3d2b9d: Waiting\n",
      "9f145ca68032: Pulling fs layer\n",
      "abce4c168c8f: Waiting\n",
      "434476c6b401: Pulling fs layer\n",
      "c05bf1206572: Waiting\n",
      "16c72bfe563b: Waiting\n",
      "0d74e44a341b: Pulling fs layer\n",
      "f7bc497efb25: Pulling fs layer\n",
      "4ee086c2d8f9: Waiting\n",
      "b9b73f1072c4: Pulling fs layer\n",
      "af67682a070e: Pulling fs layer\n",
      "f7bc497efb25: Waiting\n",
      "434476c6b401: Waiting\n",
      "b9b73f1072c4: Waiting\n",
      "a8816dce234d: Pulling fs layer\n",
      "9b8651f964db: Pulling fs layer\n",
      "3f2a9601ce6f: Pulling fs layer\n",
      "7fceef0e4dba: Pulling fs layer\n",
      "3f2a9601ce6f: Waiting\n",
      "7fceef0e4dba: Waiting\n",
      "2342a54620ff: Pulling fs layer\n",
      "1def91980a11: Pulling fs layer\n",
      "2342a54620ff: Waiting\n",
      "1def91980a11: Waiting\n",
      "e47186c04458: Pulling fs layer\n",
      "f00e669ae754: Pulling fs layer\n",
      "b9f1e77afb25: Pulling fs layer\n",
      "e47186c04458: Waiting\n",
      "f12319182589: Pulling fs layer\n",
      "5578439cd807: Pulling fs layer\n",
      "f00e669ae754: Waiting\n",
      "b9f1e77afb25: Waiting\n",
      "0661540359cb: Pulling fs layer\n",
      "e10f1743bdaf: Pulling fs layer\n",
      "f12319182589: Waiting\n",
      "5578439cd807: Waiting\n",
      "f5721f24a795: Pulling fs layer\n",
      "0661540359cb: Waiting\n",
      "e10f1743bdaf: Waiting\n",
      "504d46c2bb2a: Pulling fs layer\n",
      "f71574d9f199: Pulling fs layer\n",
      "951a7d7d79a0: Pulling fs layer\n",
      "9bfbaf26a2fa: Pulling fs layer\n",
      "f5721f24a795: Waiting\n",
      "504d46c2bb2a: Waiting\n",
      "169198376041: Pulling fs layer\n",
      "951a7d7d79a0: Waiting\n",
      "f71574d9f199: Waiting\n",
      "89d1c502bc27: Pulling fs layer\n",
      "169198376041: Waiting\n",
      "89d1c502bc27: Waiting\n",
      "d16fb958ffce: Verifying Checksum\n",
      "d16fb958ffce: Download complete\n",
      "d7c7f6dae015: Download complete\n",
      "dc1d1b858f8d: Verifying Checksum\n",
      "9736392613d1: Download complete\n",
      "6ba9ceffc16c: Verifying Checksum\n",
      "6ba9ceffc16c: Download complete\n",
      "74848a82745a: Download complete\n",
      "45feee628410: Download complete\n",
      "89a8956c6c35: Verifying Checksum\n",
      "89a8956c6c35: Download complete\n",
      "abce4c168c8f: Verifying Checksum\n",
      "abce4c168c8f: Download complete\n",
      "c05bf1206572: Verifying Checksum\n",
      "c05bf1206572: Download complete\n",
      "b863ed055d32: Download complete\n",
      "0dcd07dd2212: Verifying Checksum\n",
      "0dcd07dd2212: Download complete\n",
      "81eea7d16450: Verifying Checksum\n",
      "81eea7d16450: Download complete\n",
      "4ee086c2d8f9: Download complete\n",
      "379930b0d052: Verifying Checksum\n",
      "379930b0d052: Download complete\n",
      "b863ed055d32: Pull complete\n",
      "9f145ca68032: Verifying Checksum\n",
      "9f145ca68032: Download complete\n",
      "d7c7f6dae015: Pull complete\n",
      "d16fb958ffce: Pull complete\n",
      "434476c6b401: Verifying Checksum\n",
      "434476c6b401: Download complete\n",
      "dc1d1b858f8d: Pull complete\n",
      "16c72bfe563b: Verifying Checksum\n",
      "16c72bfe563b: Download complete\n",
      "6ba9ceffc16c: Pull complete\n",
      "9736392613d1: Pull complete\n",
      "f7bc497efb25: Verifying Checksum\n",
      "379930b0d052: Pull complete\n",
      "74848a82745a: Pull complete\n",
      "45feee628410: Pull complete\n",
      "89a8956c6c35: Pull complete\n",
      "abce4c168c8f: Pull complete\n",
      "b9b73f1072c4: Verifying Checksum\n",
      "b9b73f1072c4: Download complete\n",
      "c05bf1206572: Pull complete\n",
      "af67682a070e: Verifying Checksum\n",
      "af67682a070e: Download complete\n",
      "a8816dce234d: Verifying Checksum\n",
      "a8816dce234d: Download complete\n",
      "9b8651f964db: Verifying Checksum\n",
      "9b8651f964db: Download complete\n",
      "3f2a9601ce6f: Verifying Checksum\n",
      "3f2a9601ce6f: Download complete\n",
      "7fceef0e4dba: Verifying Checksum\n",
      "7fceef0e4dba: Download complete\n",
      "2342a54620ff: Verifying Checksum\n",
      "2342a54620ff: Download complete\n",
      "c275ba3d2b9d: Verifying Checksum\n",
      "c275ba3d2b9d: Download complete\n",
      "e47186c04458: Verifying Checksum\n",
      "e47186c04458: Download complete\n",
      "f00e669ae754: Verifying Checksum\n",
      "f00e669ae754: Download complete\n",
      "b9f1e77afb25: Verifying Checksum\n",
      "b9f1e77afb25: Download complete\n",
      "f12319182589: Verifying Checksum\n",
      "f12319182589: Download complete\n",
      "5578439cd807: Verifying Checksum\n",
      "5578439cd807: Download complete\n",
      "c275ba3d2b9d: Pull complete\n",
      "0dcd07dd2212: Pull complete\n",
      "81eea7d16450: Pull complete\n",
      "4ee086c2d8f9: Pull complete\n",
      "16c72bfe563b: Pull complete\n",
      "9f145ca68032: Pull complete\n",
      "434476c6b401: Pull complete\n",
      "0661540359cb: Verifying Checksum\n",
      "0661540359cb: Download complete\n",
      "e10f1743bdaf: Verifying Checksum\n",
      "e10f1743bdaf: Download complete\n",
      "0d74e44a341b: Verifying Checksum\n",
      "0d74e44a341b: Download complete\n",
      "504d46c2bb2a: Download complete\n",
      "f5721f24a795: Verifying Checksum\n",
      "f5721f24a795: Download complete\n",
      "951a7d7d79a0: Verifying Checksum\n",
      "951a7d7d79a0: Download complete\n",
      "f71574d9f199: Download complete\n",
      "169198376041: Verifying Checksum\n",
      "169198376041: Download complete\n",
      "9bfbaf26a2fa: Verifying Checksum\n",
      "9bfbaf26a2fa: Download complete\n",
      "89d1c502bc27: Verifying Checksum\n",
      "89d1c502bc27: Download complete\n",
      "0d74e44a341b: Pull complete\n",
      "f7bc497efb25: Pull complete\n",
      "b9b73f1072c4: Pull complete\n",
      "af67682a070e: Pull complete\n",
      "a8816dce234d: Pull complete\n",
      "9b8651f964db: Pull complete\n",
      "3f2a9601ce6f: Pull complete\n",
      "7fceef0e4dba: Pull complete\n",
      "2342a54620ff: Pull complete\n",
      "1def91980a11: Verifying Checksum\n",
      "1def91980a11: Download complete\n",
      "1def91980a11: Pull complete\n",
      "e47186c04458: Pull complete\n",
      "f00e669ae754: Pull complete\n",
      "b9f1e77afb25: Pull complete\n",
      "f12319182589: Pull complete\n",
      "5578439cd807: Pull complete\n",
      "0661540359cb: Pull complete\n",
      "e10f1743bdaf: Pull complete\n",
      "f5721f24a795: Pull complete\n",
      "504d46c2bb2a: Pull complete\n",
      "f71574d9f199: Pull complete\n",
      "951a7d7d79a0: Pull complete\n",
      "9bfbaf26a2fa: Pull complete\n",
      "169198376041: Pull complete\n",
      "89d1c502bc27: Pull complete\n",
      "Digest: sha256:64e296668d398a106f64bd840772ffb63372148b8c1170b152e7e577013661c9\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/tensorflow:19.08-py3\n",
      " ---> be978d32a5c3\n",
      "Step 3/19 : RUN apt-get update && apt-get install -y pbzip2 pv bzip2 libcurl4 curl\n",
      " ---> Running in 9d7246f44a3f\n",
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7061 B]\n",
      "Get:4 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [808 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [800 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [26.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [40.4 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1096 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1342 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.1 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4242 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Fetched 17.5 MB in 3s (5213 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "bzip2 is already the newest version (1.0.6-8.1ubuntu0.2).\n",
      "Suggested packages:\n",
      "  doc-base\n",
      "The following NEW packages will be installed:\n",
      "  pbzip2 pv\n",
      "The following packages will be upgraded:\n",
      "  curl libcurl4\n",
      "2 upgraded, 2 newly installed, 0 to remove and 54 not upgraded.\n",
      "Need to get 458 kB of archives.\n",
      "After this operation, 220 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 curl amd64 7.58.0-2ubuntu3.8 [159 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcurl4 amd64 7.58.0-2ubuntu3.8 [214 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pbzip2 amd64 1.1.9-1build1 [37.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 pv amd64 1.6.6-1 [48.3 kB]\n",
      "\u001b[91mdebconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\u001b[0m\u001b[91mdebconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "\u001b[0m\u001b[91mdpkg-preconfigure: unable to re-open stdin: \n",
      "\u001b[0mFetched 458 kB in 1s (418 kB/s)\n",
      "(Reading database ... \r",
      "(Reading database ... 5%\r",
      "(Reading database ... 10%\r",
      "(Reading database ... 15%\r",
      "(Reading database ... 20%\r",
      "(Reading database ... 25%\r",
      "(Reading database ... 30%\r",
      "(Reading database ... 35%\r",
      "(Reading database ... 40%\r",
      "(Reading database ... 45%\r",
      "(Reading database ... 50%\r",
      "(Reading database ... 55%\r",
      "(Reading database ... 60%\r",
      "(Reading database ... 65%\r",
      "(Reading database ... 70%\r",
      "(Reading database ... 75%\r",
      "(Reading database ... 80%\r",
      "(Reading database ... 85%\r",
      "(Reading database ... 90%\r",
      "(Reading database ... 95%\r",
      "(Reading database ... 100%\r",
      "(Reading database ... 34622 files and directories currently installed.)\r\n",
      "Preparing to unpack .../curl_7.58.0-2ubuntu3.8_amd64.deb ...\r\n",
      "Unpacking curl (7.58.0-2ubuntu3.8) over (7.58.0-2ubuntu3.7) ...\r\n",
      "Preparing to unpack .../libcurl4_7.58.0-2ubuntu3.8_amd64.deb ...\r\n",
      "Unpacking libcurl4:amd64 (7.58.0-2ubuntu3.8) over (7.58.0-2ubuntu3.7) ...\r\n",
      "Selecting previously unselected package pbzip2.\r\n",
      "Preparing to unpack .../pbzip2_1.1.9-1build1_amd64.deb ...\r\n",
      "Unpacking pbzip2 (1.1.9-1build1) ...\r\n",
      "Selecting previously unselected package pv.\r\n",
      "Preparing to unpack .../archives/pv_1.6.6-1_amd64.deb ...\r\n",
      "Unpacking pv (1.6.6-1) ...\r\n",
      "Setting up pv (1.6.6-1) ...\r\n",
      "Setting up libcurl4:amd64 (7.58.0-2ubuntu3.8) ...\r\n",
      "Setting up pbzip2 (1.1.9-1build1) ...\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
      "Setting up curl (7.58.0-2ubuntu3.8) ...\r\n",
      "Removing intermediate container 9d7246f44a3f\n",
      " ---> 739e57ec3a3e\n",
      "Step 4/19 : RUN pip install toposort networkx pytest nltk tqdm html2text progressbar\n",
      " ---> Running in 20afb73d1acb\n",
      "Collecting toposort\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Collecting networkx\n",
      "  Downloading https://files.pythonhosted.org/packages/41/8f/dd6a8e85946def36e4f2c69c84219af0fa5e832b018c970e92f2ad337e45/networkx-2.4-py3-none-any.whl (1.6MB)\n",
      "Collecting pytest\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/c0/34033b2df7718b91c667bd259d5ce632ec3720198b7068c0ba6f6104ff89/pytest-5.3.5-py3-none-any.whl (235kB)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.35.0)\n",
      "Collecting html2text\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
      "Collecting progressbar\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.0)\n",
      "Collecting more-itertools>=4.0.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/72/96/4297306cc270eef1e3461da034a3bebe7c84eff052326b130824e98fc3fb/more_itertools-8.2.0-py3-none-any.whl (43kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest) (19.1.0)\n",
      "Collecting packaging (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/98/42/87c585dd3b113c775e65fd6b8d9d0a43abe1819c471d7af702d4e01e9b20/packaging-20.1-py2.py3-none-any.whl\n",
      "Collecting py>=1.5.0 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/99/8d/21e1767c009211a62a8e3067280bfce76e89c9f876180308515942304d2d/py-1.8.1-py2.py3-none-any.whl (83kB)\n",
      "Collecting importlib-metadata>=0.12; python_version < \"3.8\" (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/8b/03/a00d504808808912751e64ccf414be53c29cad620e3de2421135fcae3025/importlib_metadata-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest) (0.1.7)\n",
      "Collecting pluggy<1.0,>=0.12 (from pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/28/85c7aa31b80d150b772fbe4a229487bc6644da9ccb7e427dd8cc60cb8a62/pluggy-0.13.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest) (2.4.2)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest)\n",
      "  Downloading https://files.pythonhosted.org/packages/be/69/4ac28bf238f287f1677f41392e24d2c4ffafcf11648c23824f5f62ef6ccb/zipp-2.1.0-py3-none-any.whl\n",
      "Building wheels for collected packages: progressbar\n",
      "  Building wheel for progressbar (setup.py): started\n",
      "  Building wheel for progressbar (setup.py): finished with status 'done'\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=9104ca8e5cab1278799f005eada62488e6a73ee9140c01c16657a1b1db2da7c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
      "Successfully built progressbar\n",
      "Installing collected packages: toposort, networkx, more-itertools, packaging, py, zipp, importlib-metadata, pluggy, pytest, html2text, progressbar\n",
      "Successfully installed html2text-2020.1.16 importlib-metadata-1.5.0 more-itertools-8.2.0 networkx-2.4 packaging-20.1 pluggy-0.13.1 progressbar-2.5 py-1.8.1 pytest-5.3.5 toposort-1.5 zipp-2.1.0\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 20afb73d1acb\n",
      " ---> 63e514dff55f\n",
      "Step 5/19 : WORKDIR /workspace\n",
      " ---> Running in 512319b895e1\n",
      "Removing intermediate container 512319b895e1\n",
      " ---> 99d5962f5c20\n",
      "Step 6/19 : RUN git clone https://github.com/openai/gradient-checkpointing.git\n",
      " ---> Running in c943bba5cc3c\n",
      "\u001b[91mCloning into 'gradient-checkpointing'...\n",
      "\u001b[0mRemoving intermediate container c943bba5cc3c\n",
      " ---> 2a1c04fd1d85\n",
      "Step 7/19 : RUN git clone https://github.com/attardi/wikiextractor.git\n",
      " ---> Running in 59f5391617c3\n",
      "\u001b[91mCloning into 'wikiextractor'...\n",
      "\u001b[0mRemoving intermediate container 59f5391617c3\n",
      " ---> 95c7d08027cd\n",
      "Step 8/19 : RUN git clone https://github.com/soskek/bookcorpus.git\n",
      " ---> Running in af2e421fcbcc\n",
      "\u001b[91mCloning into 'bookcorpus'...\n",
      "\u001b[0mRemoving intermediate container af2e421fcbcc\n",
      " ---> e6aeaa3beb5e\n",
      "Step 9/19 : RUN git clone https://github.com/titipata/pubmed_parser\n",
      " ---> Running in 7683ce9b5f15\n",
      "\u001b[91mCloning into 'pubmed_parser'...\n",
      "\u001b[0mRemoving intermediate container 7683ce9b5f15\n",
      " ---> 3b28fcdb2739\n",
      "Step 10/19 : RUN pip3 install /workspace/pubmed_parser\n",
      " ---> Running in 18a2f6b4a224\n",
      "Processing ./pubmed_parser\n",
      "Collecting lxml (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/ba/a0e6866057fc0bbd17192925c1d63a3b85cf522965de9bc02364d08e5b84/lxml-4.5.0-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "Collecting unidecode (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "Collecting requests (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pubmed-parser==0.2.1) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pubmed-parser==0.2.1) (1.14.5)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pubmed-parser==0.2.1) (5.3.5)\n",
      "Collecting pytest-cov (from pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/54/3673ee8be482f81527678ac894276223b9814bb7262e4f730469bb7bf70e/pytest_cov-2.8.1-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Collecting idna<2.9,>=2.5 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (19.1.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (20.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (8.2.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (1.8.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (1.5.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest->pubmed-parser==0.2.1) (0.13.1)\n",
      "Collecting coverage>=4.4 (from pytest-cov->pubmed-parser==0.2.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/58/df86640436e28ca46fa5e93e603c82abb0cba309b37cbaec1e1188ea53a6/coverage-5.0.3-cp36-cp36m-manylinux1_x86_64.whl (227kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest->pubmed-parser==0.2.1) (2.4.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest->pubmed-parser==0.2.1) (2.1.0)\n",
      "Building wheels for collected packages: pubmed-parser\n",
      "  Building wheel for pubmed-parser (setup.py): started\n",
      "  Building wheel for pubmed-parser (setup.py): finished with status 'done'\n",
      "  Created wheel for pubmed-parser: filename=pubmed_parser-0.2.1-cp36-none-any.whl size=18167 sha256=1f63225dd7c0e5498195bbb17a5cfda594140c0215616e285f9a98b33246d9fe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ok52w_qb/wheels/70/0e/94/406257b015fc1ba650bee2b5e3fd979b281504f67008d482f3\n",
      "Successfully built pubmed-parser\n",
      "Installing collected packages: lxml, unidecode, urllib3, certifi, idna, chardet, requests, coverage, pytest-cov, pubmed-parser\n",
      "Successfully installed certifi-2019.11.28 chardet-3.0.4 coverage-5.0.3 idna-2.8 lxml-4.5.0 pubmed-parser-0.2.1 pytest-cov-2.8.1 requests-2.22.0 unidecode-1.1.1 urllib3-1.25.8\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 18a2f6b4a224\n",
      " ---> 805fa96ceb85\n",
      "Step 11/19 : ARG TRTIS_CLIENTS_URL=https://github.com/NVIDIA/tensorrt-inference-server/releases/download/v1.5.0/v1.5.0_ubuntu1804.clients.tar.gz\n",
      " ---> Running in 6e9c45d6b30c\n",
      "Removing intermediate container 6e9c45d6b30c\n",
      " ---> 922d8d4a0a83\n",
      "Step 12/19 : RUN mkdir -p /workspace/install     && curl -L ${TRTIS_CLIENTS_URL} | tar xvz -C /workspace/install\n",
      " ---> Running in 346eeaaf1357\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     \u001b[0m\u001b[91m0    0     0    0     0      0\u001b[0m\u001b[91m      0 --:--:-- --:-\u001b[0m\u001b[91m-:-- --:--:--     0\u001b[0m\u001b[91m\r",
      "100   623    0   623    0     0   2462      0 --:--:-- --:--:-- --:--:--  2462\n",
      "\u001b[0mbin/\n",
      "bin/ensemble_image_client\n",
      "bin/simple_client\n",
      "\u001b[91m\r",
      "  1 4081k    1 50725    0     0  66743      0  0:01:02 --:--:--  0:01:02 66743\u001b[0mbin/simple_sequence_client\n",
      "bin/image_client\n",
      "bin/perf_client\n",
      "bin/simple_string_client\n",
      "bin/simple_callback_client\n",
      "include/\n",
      "include/model_config.pb.h\n",
      "include/request_status.pb.h\n",
      "include/api.pb.h\n",
      "include/request_http.h\n",
      "include/request.h\n",
      "include/request_grpc.h\n",
      "include/server_status.pb.h\n",
      "lib/\n",
      "lib/librequest.so\n",
      "python/\n",
      "python/simple_string_client.py\n",
      "python/simple_callback_client.py\n",
      "python/simple_sequence_client.py\n",
      "python/grpc_image_client.py\n",
      "python/tensorrtserver-1.5.0-py2.py3-none-linux_x86_64.whl\n",
      "\u001b[91m\r",
      " 90 4081k   90 3688k    0     0  2171k      0  0:00:01  0:00:01 --:--:-- 3871k\u001b[0m\u001b[91m\r",
      "100 4081k  100 4081k    0     0  2298k      0  0:00:01  0:00:01 --:--:-- 3968k\n",
      "\u001b[0mpython/simple_client.py\n",
      "python/image_client.py\n",
      "python/ensemble_image_client.py\n",
      "Removing intermediate container 346eeaaf1357\n",
      " ---> d05ebe8c06ff\n",
      "Step 13/19 : RUN pip install /workspace/install/python/tensorrtserver*.whl\n",
      " ---> Running in dae0e4a01e3b\n",
      "Processing ./install/python/tensorrtserver-1.5.0-py2.py3-none-linux_x86_64.whl\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (3.9.1)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (0.17.1)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (1.22.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorrtserver==1.5.0) (1.14.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.5.0->tensorrtserver==1.5.0) (41.0.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.5.0->tensorrtserver==1.5.0) (1.12.0)\n",
      "Installing collected packages: tensorrtserver\n",
      "Successfully installed tensorrtserver-1.5.0\n",
      "\u001b[91mWARNING: You are using pip version 19.2.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container dae0e4a01e3b\n",
      " ---> 20ff9675cdd7\n",
      "Step 14/19 : WORKDIR /workspace/bert\n",
      " ---> Running in dbae1acb266a\n",
      "Removing intermediate container dbae1acb266a\n",
      " ---> bfcc55348544\n",
      "Step 15/19 : COPY . .\n",
      " ---> b316fd838f53\n",
      "Step 16/19 : ENV PYTHONPATH /workspace/bert\n",
      " ---> Running in 81ba1f1cb036\n",
      "Removing intermediate container 81ba1f1cb036\n",
      " ---> 76297c2e4fd8\n",
      "Step 17/19 : ENV BERT_PREP_WORKING_DIR /workspace/bert/data\n",
      " ---> Running in 2d142c22339f\n",
      "Removing intermediate container 2d142c22339f\n",
      " ---> dff32e6df5d7\n",
      "Step 18/19 : ENV PATH //workspace/install/bin:${PATH}\n",
      " ---> Running in 0b69b7e0f6d0\n",
      "Removing intermediate container 0b69b7e0f6d0\n",
      " ---> 8b4be7c39e19\n",
      "Step 19/19 : ENV LD_LIBRARY_PATH /workspace/install/lib:${LD_LIBRARY_PATH}\n",
      " ---> Running in d6fedc5d4436\n",
      "Removing intermediate container d6fedc5d4436\n",
      " ---> 2a395a35bff7\n",
      "Successfully built 2a395a35bff7\n",
      "Successfully tagged bert:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You need to run this command from the toplevel of the working tree.\n",
      "./scripts/docker/build.sh: line 7: cd: tensorrt-inference-server: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "cd ..\n",
    "bash ./scripts/docker/build.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BERT TRTIS performance (1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      "TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use 'nvidia-docker run' to start this container; see\r\n",
      "   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\r\n",
      "\r\n",
      "ERROR: Detected MOFED driver 4.7-1.0.0, but this container has version 4.4-1.0.0.\r\n",
      "       Unable to automatically upgrade this container.\r\n",
      "       Use of RDMA for multi-node communication will be unreliable.\r\n",
      "\r\n",
      "NOTE: MOFED driver was detected, but nv_peer_mem driver was not detected.\r\n",
      "      Multi-node communication performance may be reduced.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 500 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 346 infer/sec. Avg latency: 2877 usec (std 134 usec)\r\n",
      "  Pass [2] throughput: 347 infer/sec. Avg latency: 2865 usec (std 234 usec)\r\n",
      "  Pass [3] throughput: 342 infer/sec. Avg latency: 2906 usec (std 114 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1028\r\n",
      "    Throughput: 342 infer/sec\r\n",
      "    Avg latency: 2906 usec (standard deviation 114 usec)\r\n",
      "    p50 latency: 2911 usec\r\n",
      "    p90 latency: 3063 usec\r\n",
      "    p95 latency: 3094 usec\r\n",
      "    p99 latency: 3146 usec\r\n",
      "    Avg gRPC time: 2835 usec (marshal 6 usec + response wait 2820 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1238\r\n",
      "    Avg request latency: 2298 usec (overhead 12 usec + queue 44 usec + compute 2242 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 442 infer/sec. Avg latency: 4513 usec (std 74 usec)\r\n",
      "  Pass [2] throughput: 439 infer/sec. Avg latency: 4538 usec (std 70 usec)\r\n",
      "  Pass [3] throughput: 438 infer/sec. Avg latency: 4553 usec (std 69 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1314\r\n",
      "    Throughput: 438 infer/sec\r\n",
      "    Avg latency: 4553 usec (standard deviation 69 usec)\r\n",
      "    p50 latency: 4550 usec\r\n",
      "    p90 latency: 4635 usec\r\n",
      "    p95 latency: 4659 usec\r\n",
      "    p99 latency: 4717 usec\r\n",
      "    Avg gRPC time: 4490 usec (marshal 6 usec + response wait 4475 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1578\r\n",
      "    Avg request latency: 3924 usec (overhead 12 usec + queue 1692 usec + compute 2220 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 435 infer/sec. Avg latency: 6874 usec (std 81 usec)\r\n",
      "  Pass [2] throughput: 435 infer/sec. Avg latency: 6871 usec (std 629 usec)\r\n",
      "  Pass [3] throughput: 440 infer/sec. Avg latency: 6800 usec (std 74 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1321\r\n",
      "    Throughput: 440 infer/sec\r\n",
      "    Avg latency: 6800 usec (standard deviation 74 usec)\r\n",
      "    p50 latency: 6794 usec\r\n",
      "    p90 latency: 6884 usec\r\n",
      "    p95 latency: 6912 usec\r\n",
      "    p99 latency: 6996 usec\r\n",
      "    Avg gRPC time: 6733 usec (marshal 6 usec + response wait 6717 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1586\r\n",
      "    Avg request latency: 6135 usec (overhead 10 usec + queue 3913 usec + compute 2212 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 440 infer/sec. Avg latency: 9070 usec (std 70 usec)\r\n",
      "  Pass [2] throughput: 440 infer/sec. Avg latency: 9069 usec (std 70 usec)\r\n",
      "  Pass [3] throughput: 439 infer/sec. Avg latency: 9082 usec (std 75 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1319\r\n",
      "    Throughput: 439 infer/sec\r\n",
      "    Avg latency: 9082 usec (standard deviation 75 usec)\r\n",
      "    p50 latency: 9081 usec\r\n",
      "    p90 latency: 9177 usec\r\n",
      "    p95 latency: 9207 usec\r\n",
      "    p99 latency: 9275 usec\r\n",
      "    Avg gRPC time: 9025 usec (marshal 6 usec + response wait 9009 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1584\r\n",
      "    Avg request latency: 8458 usec (overhead 12 usec + queue 6232 usec + compute 2214 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 11439 usec (std 119 usec)\r\n",
      "  Pass [2] throughput: 436 infer/sec. Avg latency: 11450 usec (std 99 usec)\r\n",
      "  Pass [3] throughput: 434 infer/sec. Avg latency: 11503 usec (std 97 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1302\r\n",
      "    Throughput: 434 infer/sec\r\n",
      "    Avg latency: 11503 usec (standard deviation 97 usec)\r\n",
      "    p50 latency: 11492 usec\r\n",
      "    p90 latency: 11615 usec\r\n",
      "    p95 latency: 11673 usec\r\n",
      "    p99 latency: 11801 usec\r\n",
      "    Avg gRPC time: 11440 usec (marshal 6 usec + response wait 11424 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1563\r\n",
      "    Avg request latency: 10844 usec (overhead 10 usec + queue 8601 usec + compute 2233 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 435 infer/sec. Avg latency: 13774 usec (std 116 usec)\r\n",
      "  Pass [2] throughput: 433 infer/sec. Avg latency: 13839 usec (std 134 usec)\r\n",
      "  Pass [3] throughput: 432 infer/sec. Avg latency: 13877 usec (std 113 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1296\r\n",
      "    Throughput: 432 infer/sec\r\n",
      "    Avg latency: 13877 usec (standard deviation 113 usec)\r\n",
      "    p50 latency: 13867 usec\r\n",
      "    p90 latency: 13992 usec\r\n",
      "    p95 latency: 14039 usec\r\n",
      "    p99 latency: 14368 usec\r\n",
      "    Avg gRPC time: 13831 usec (marshal 7 usec + response wait 13814 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1555\r\n",
      "    Avg request latency: 13210 usec (overhead 12 usec + queue 10953 usec + compute 2245 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 437 infer/sec. Avg latency: 16000 usec (std 110 usec)\r\n",
      "  Pass [2] throughput: 437 infer/sec. Avg latency: 15987 usec (std 104 usec)\r\n",
      "  Pass [3] throughput: 437 infer/sec. Avg latency: 16005 usec (std 122 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1311\r\n",
      "    Throughput: 437 infer/sec\r\n",
      "    Avg latency: 16005 usec (standard deviation 122 usec)\r\n",
      "    p50 latency: 15996 usec\r\n",
      "    p90 latency: 16124 usec\r\n",
      "    p95 latency: 16177 usec\r\n",
      "    p99 latency: 16512 usec\r\n",
      "    Avg gRPC time: 15944 usec (marshal 6 usec + response wait 15928 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1573\r\n",
      "    Avg request latency: 15344 usec (overhead 9 usec + queue 13106 usec + compute 2229 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 439 infer/sec. Avg latency: 18211 usec (std 108 usec)\r\n",
      "  Pass [2] throughput: 433 infer/sec. Avg latency: 18426 usec (std 169 usec)\r\n",
      "  Pass [3] throughput: 431 infer/sec. Avg latency: 18519 usec (std 135 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1295\r\n",
      "    Throughput: 431 infer/sec\r\n",
      "    Avg latency: 18519 usec (standard deviation 135 usec)\r\n",
      "    p50 latency: 18503 usec\r\n",
      "    p90 latency: 18683 usec\r\n",
      "    p95 latency: 18754 usec\r\n",
      "    p99 latency: 18992 usec\r\n",
      "    Avg gRPC time: 18455 usec (marshal 6 usec + response wait 18439 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1554\r\n",
      "    Avg request latency: 17855 usec (overhead 11 usec + queue 15605 usec + compute 2239 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 432 infer/sec. Avg latency: 20799 usec (std 149 usec)\r\n",
      "  Pass [2] throughput: 432 infer/sec. Avg latency: 20804 usec (std 139 usec)\r\n",
      "  Pass [3] throughput: 431 infer/sec. Avg latency: 20873 usec (std 134 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1293\r\n",
      "    Throughput: 431 infer/sec\r\n",
      "    Avg latency: 20873 usec (standard deviation 134 usec)\r\n",
      "    p50 latency: 20856 usec\r\n",
      "    p90 latency: 21034 usec\r\n",
      "    p95 latency: 21101 usec\r\n",
      "    p99 latency: 21323 usec\r\n",
      "    Avg gRPC time: 20829 usec (marshal 7 usec + response wait 20812 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1550\r\n",
      "    Avg request latency: 20271 usec (overhead 9 usec + queue 18010 usec + compute 2252 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 431 infer/sec. Avg latency: 23156 usec (std 130 usec)\r\n",
      "  Pass [2] throughput: 432 infer/sec. Avg latency: 23111 usec (std 129 usec)\r\n",
      "  Pass [3] throughput: 432 infer/sec. Avg latency: 23099 usec (std 108 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1298\r\n",
      "    Throughput: 432 infer/sec\r\n",
      "    Avg latency: 23099 usec (standard deviation 108 usec)\r\n",
      "    p50 latency: 23094 usec\r\n",
      "    p90 latency: 23240 usec\r\n",
      "    p95 latency: 23285 usec\r\n",
      "    p99 latency: 23346 usec\r\n",
      "    Avg gRPC time: 23037 usec (marshal 6 usec + response wait 23021 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1558\r\n",
      "    Avg request latency: 22468 usec (overhead 11 usec + queue 20215 usec + compute 2242 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 342 infer/sec, latency 2906 usec\r\n",
      "Concurrency: 2, 438 infer/sec, latency 4553 usec\r\n",
      "Concurrency: 3, 440 infer/sec, latency 6800 usec\r\n",
      "Concurrency: 4, 439 infer/sec, latency 9082 usec\r\n",
      "Concurrency: 5, 434 infer/sec, latency 11503 usec\r\n",
      "Concurrency: 6, 432 infer/sec, latency 13877 usec\r\n",
      "Concurrency: 7, 437 infer/sec, latency 16005 usec\r\n",
      "Concurrency: 8, 431 infer/sec, latency 18519 usec\r\n",
      "Concurrency: 9, 431 infer/sec, latency 20873 usec\r\n",
      "Concurrency: 10, 432 infer/sec, latency 23099 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"base\" \"128\" \"fp16\" \"1\" \"1\" \"500\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"base\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "cd ..\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model reconfiguration & updated inference performance - 2 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU의 구성을 바꾸기 위해 model repository에 있는 GPU 정보를 새롭게 업데이트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_base_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0, 1]\n",
    "        profile: \"0\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 TensorRT Inference Server에서는 이를 감지하여 새로 GPU instance를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      "...........TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 시점에서 nvidia-smi 등을 통해서 GPU 메모리 사용량을 보시면, 4개의 GPU에 메모리 사용량이 늘어난 것을 보실 수 있습니다.\n",
    "\n",
    "이제 테스트를 해볼 차례입니다. 다만 여기서는 Client 단의 max latency를 기존의 500에서 5000으로 조정하도록 하겠습니다. (이유 파악중..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use 'nvidia-docker run' to start this container; see\r\n",
      "   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\r\n",
      "\r\n",
      "ERROR: Detected MOFED driver 4.7-1.0.0, but this container has version 4.4-1.0.0.\r\n",
      "       Unable to automatically upgrade this container.\r\n",
      "       Use of RDMA for multi-node communication will be unreliable.\r\n",
      "\r\n",
      "NOTE: MOFED driver was detected, but nv_peer_mem driver was not detected.\r\n",
      "      Multi-node communication performance may be reduced.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 5000 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 238 infer/sec. Avg latency: 4180 usec (std 411 usec)\r\n",
      "  Pass [2] throughput: 271 infer/sec. Avg latency: 3675 usec (std 102 usec)\r\n",
      "  Pass [3] throughput: 272 infer/sec. Avg latency: 3657 usec (std 107 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 818\r\n",
      "    Throughput: 272 infer/sec\r\n",
      "    Avg latency: 3657 usec (standard deviation 107 usec)\r\n",
      "    p50 latency: 3652 usec\r\n",
      "    p90 latency: 3799 usec\r\n",
      "    p95 latency: 3842 usec\r\n",
      "    p99 latency: 3892 usec\r\n",
      "    Avg gRPC time: 3606 usec (marshal 6 usec + response wait 3591 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 981\r\n",
      "    Avg request latency: 3047 usec (overhead 12 usec + queue 58 usec + compute 2977 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 677 infer/sec. Avg latency: 2941 usec (std 143 usec)\r\n",
      "  Pass [2] throughput: 685 infer/sec. Avg latency: 2905 usec (std 128 usec)\r\n",
      "  Pass [3] throughput: 671 infer/sec. Avg latency: 2964 usec (std 131 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2015\r\n",
      "    Throughput: 671 infer/sec\r\n",
      "    Avg latency: 2964 usec (standard deviation 131 usec)\r\n",
      "    p50 latency: 2958 usec\r\n",
      "    p90 latency: 3132 usec\r\n",
      "    p95 latency: 3191 usec\r\n",
      "    p99 latency: 3278 usec\r\n",
      "    Avg gRPC time: 2887 usec (marshal 6 usec + response wait 2872 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 2430\r\n",
      "    Avg request latency: 2318 usec (overhead 10 usec + queue 56 usec + compute 2252 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 873 infer/sec. Avg latency: 3422 usec (std 352 usec)\r\n",
      "  Pass [2] throughput: 871 infer/sec. Avg latency: 3431 usec (std 422 usec)\r\n",
      "  Pass [3] throughput: 870 infer/sec. Avg latency: 3436 usec (std 344 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2610\r\n",
      "    Throughput: 870 infer/sec\r\n",
      "    Avg latency: 3436 usec (standard deviation 344 usec)\r\n",
      "    p50 latency: 3414 usec\r\n",
      "    p90 latency: 3851 usec\r\n",
      "    p95 latency: 3925 usec\r\n",
      "    p99 latency: 4139 usec\r\n",
      "    Avg gRPC time: 3376 usec (marshal 6 usec + response wait 3361 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3137\r\n",
      "    Avg request latency: 2854 usec (overhead 11 usec + queue 615 usec + compute 2228 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 869 infer/sec. Avg latency: 4590 usec (std 99 usec)\r\n",
      "  Pass [2] throughput: 867 infer/sec. Avg latency: 4600 usec (std 103 usec)\r\n",
      "  Pass [3] throughput: 867 infer/sec. Avg latency: 4597 usec (std 87 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2603\r\n",
      "    Throughput: 867 infer/sec\r\n",
      "    Avg latency: 4597 usec (standard deviation 87 usec)\r\n",
      "    p50 latency: 4594 usec\r\n",
      "    p90 latency: 4670 usec\r\n",
      "    p95 latency: 4694 usec\r\n",
      "    p99 latency: 4760 usec\r\n",
      "    Avg gRPC time: 4539 usec (marshal 6 usec + response wait 4524 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3123\r\n",
      "    Avg request latency: 4028 usec (overhead 11 usec + queue 1780 usec + compute 2237 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 867 infer/sec. Avg latency: 5749 usec (std 522 usec)\r\n",
      "  Pass [2] throughput: 870 infer/sec. Avg latency: 5732 usec (std 421 usec)\r\n",
      "  Pass [3] throughput: 869 infer/sec. Avg latency: 5739 usec (std 480 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2609\r\n",
      "    Throughput: 869 infer/sec\r\n",
      "    Avg latency: 5739 usec (standard deviation 480 usec)\r\n",
      "    p50 latency: 5725 usec\r\n",
      "    p90 latency: 6302 usec\r\n",
      "    p95 latency: 6652 usec\r\n",
      "    p99 latency: 6839 usec\r\n",
      "    Avg gRPC time: 5679 usec (marshal 6 usec + response wait 5664 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3130\r\n",
      "    Avg request latency: 5158 usec (overhead 12 usec + queue 2912 usec + compute 2234 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 871 infer/sec. Avg latency: 6872 usec (std 101 usec)\r\n",
      "  Pass [2] throughput: 872 infer/sec. Avg latency: 6870 usec (std 79 usec)\r\n",
      "  Pass [3] throughput: 869 infer/sec. Avg latency: 6891 usec (std 75 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2608\r\n",
      "    Throughput: 869 infer/sec\r\n",
      "    Avg latency: 6891 usec (standard deviation 75 usec)\r\n",
      "    p50 latency: 6887 usec\r\n",
      "    p90 latency: 6979 usec\r\n",
      "    p95 latency: 7010 usec\r\n",
      "    p99 latency: 7099 usec\r\n",
      "    Avg gRPC time: 6839 usec (marshal 6 usec + response wait 6824 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3129\r\n",
      "    Avg request latency: 6315 usec (overhead 11 usec + queue 4069 usec + compute 2235 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 867 infer/sec. Avg latency: 8062 usec (std 504 usec)\r\n",
      "  Pass [2] throughput: 863 infer/sec. Avg latency: 8091 usec (std 475 usec)\r\n",
      "  Pass [3] throughput: 868 infer/sec. Avg latency: 8048 usec (std 626 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2606\r\n",
      "    Throughput: 868 infer/sec\r\n",
      "    Avg latency: 8048 usec (standard deviation 626 usec)\r\n",
      "    p50 latency: 8011 usec\r\n",
      "    p90 latency: 8985 usec\r\n",
      "    p95 latency: 9094 usec\r\n",
      "    p99 latency: 9226 usec\r\n",
      "    Avg gRPC time: 7981 usec (marshal 6 usec + response wait 7966 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3128\r\n",
      "    Avg request latency: 7456 usec (overhead 12 usec + queue 5205 usec + compute 2239 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 866 infer/sec. Avg latency: 9220 usec (std 152 usec)\r\n",
      "  Pass [2] throughput: 870 infer/sec. Avg latency: 9182 usec (std 129 usec)\r\n",
      "  Pass [3] throughput: 867 infer/sec. Avg latency: 9211 usec (std 143 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2602\r\n",
      "    Throughput: 867 infer/sec\r\n",
      "    Avg latency: 9211 usec (standard deviation 143 usec)\r\n",
      "    p50 latency: 9194 usec\r\n",
      "    p90 latency: 9328 usec\r\n",
      "    p95 latency: 9383 usec\r\n",
      "    p99 latency: 9772 usec\r\n",
      "    Avg gRPC time: 9155 usec (marshal 6 usec + response wait 9140 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3122\r\n",
      "    Avg request latency: 8634 usec (overhead 10 usec + queue 6382 usec + compute 2242 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 869 infer/sec. Avg latency: 10341 usec (std 445 usec)\r\n",
      "  Pass [2] throughput: 865 infer/sec. Avg latency: 10393 usec (std 416 usec)\r\n",
      "  Pass [3] throughput: 861 infer/sec. Avg latency: 10438 usec (std 574 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2584\r\n",
      "    Throughput: 861 infer/sec\r\n",
      "    Avg latency: 10438 usec (standard deviation 574 usec)\r\n",
      "    p50 latency: 10396 usec\r\n",
      "    p90 latency: 11210 usec\r\n",
      "    p95 latency: 11339 usec\r\n",
      "    p99 latency: 11642 usec\r\n",
      "    Avg gRPC time: 10375 usec (marshal 6 usec + response wait 10360 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3102\r\n",
      "    Avg request latency: 9807 usec (overhead 10 usec + queue 7547 usec + compute 2250 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 863 infer/sec. Avg latency: 11558 usec (std 215 usec)\r\n",
      "  Pass [2] throughput: 864 infer/sec. Avg latency: 11554 usec (std 188 usec)\r\n",
      "  Pass [3] throughput: 864 infer/sec. Avg latency: 11557 usec (std 152 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2593\r\n",
      "    Throughput: 864 infer/sec\r\n",
      "    Avg latency: 11557 usec (standard deviation 152 usec)\r\n",
      "    p50 latency: 11544 usec\r\n",
      "    p90 latency: 11658 usec\r\n",
      "    p95 latency: 11700 usec\r\n",
      "    p99 latency: 12039 usec\r\n",
      "    Avg gRPC time: 11495 usec (marshal 6 usec + response wait 11480 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3113\r\n",
      "    Avg request latency: 10979 usec (overhead 12 usec + queue 8719 usec + compute 2248 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 272 infer/sec, latency 3657 usec\r\n",
      "Concurrency: 2, 671 infer/sec, latency 2964 usec\r\n",
      "Concurrency: 3, 870 infer/sec, latency 3436 usec\r\n",
      "Concurrency: 4, 867 infer/sec, latency 4597 usec\r\n",
      "Concurrency: 5, 869 infer/sec, latency 5739 usec\r\n",
      "Concurrency: 6, 869 infer/sec, latency 6891 usec\r\n",
      "Concurrency: 7, 868 infer/sec, latency 8048 usec\r\n",
      "Concurrency: 8, 867 infer/sec, latency 9211 usec\r\n",
      "Concurrency: 9, 861 infer/sec, latency 10438 usec\r\n",
      "Concurrency: 10, 864 infer/sec, latency 11557 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"base\" \"128\" \"fp16\" \"1\" \"1\" \"5000\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"base\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "cd ..\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다. 이제 BERT 모델을 GPU를 이용하여 최적의 성능으로 Inference를 하실 수 있게 되셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Closing TRTIS container\n",
    "\n",
    "이제 아래 명령을 통해 실행한 container를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trt_server_cont\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f trt_server_cont"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
